<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Omni V1.5 | INCLUSION AI</title><meta name=keywords content><meta name=description content="【英文版待更新】 GITHUB Ming-Lite-Omni V1.5 整体评测结果 复杂文档理解 图文及体验 定义视频理解新标杆 在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 Ming-Omni-Lite 在多项核心视频理解基准测试中取得了突破性进展。
性能 我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：
评测基准 Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 技术背后 Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：
高效的时空建模器： 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。 高质量、多样化的视频-文本对齐数据： 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。 创新的训练目标与课程学习： 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。 迈向更智能的视频交互 Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理复杂、长时间、信息密集视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.c89f196d8672b69cf34afeba27d213af1d6900c106ff9315c4a62af441df5c8a.css integrity="sha256-yJ8ZbYZytpzzSv66J9ITrx1pAMEG/5MVxKYq9EHfXIo=" rel="preload stylesheet" as=style><link rel=icon href=https://Biao-Gong.github.io/favicon.png><link rel=apple-touch-icon href=https://Biao-Gong.github.io/favicon.png><link rel=manifest href=https://Biao-Gong.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-Lite-Omni V1.5"><meta property="og:description" content="【英文版待更新】 GITHUB Ming-Lite-Omni V1.5 整体评测结果 复杂文档理解 图文及体验 定义视频理解新标杆 在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 Ming-Omni-Lite 在多项核心视频理解基准测试中取得了突破性进展。
性能 我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：
评测基准 Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 技术背后 Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：
高效的时空建模器： 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。 高质量、多样化的视频-文本对齐数据： 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。 创新的训练目标与课程学习： 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。 迈向更智能的视频交互 Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理复杂、长时间、信息密集视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。"><meta property="og:type" content="article"><meta property="og:url" content="https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-14T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-14T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Omni V1.5"><meta name=twitter:description content="【英文版待更新】 GITHUB Ming-Lite-Omni V1.5 整体评测结果 复杂文档理解 图文及体验 定义视频理解新标杆 在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 Ming-Omni-Lite 在多项核心视频理解基准测试中取得了突破性进展。
性能 我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：
评测基准 Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 技术背后 Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：
高效的时空建模器： 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。 高质量、多样化的视频-文本对齐数据： 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。 创新的训练目标与课程学习： 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。 迈向更智能的视频交互 Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理复杂、长时间、信息密集视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://Biao-Gong.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Omni V1.5","item":"https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Omni V1.5","name":"Ming-Lite-Omni V1.5","description":"【英文版待更新】 GITHUB Ming-Lite-Omni V1.5 整体评测结果 复杂文档理解 图文及体验 定义视频理解新标杆 在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 Ming-Omni-Lite 在多项核心视频理解基准测试中取得了突破性进展。\n性能 我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：\n评测基准 Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 技术背后 Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：\n高效的时空建模器： 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。 高质量、多样化的视频-文本对齐数据： 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。 创新的训练目标与课程学习： 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。 迈向更智能的视频交互 Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理复杂、长时间、信息密集视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。","keywords":[],"articleBody":"【英文版待更新】 GITHUB Ming-Lite-Omni V1.5 整体评测结果 复杂文档理解 图文及体验 定义视频理解新标杆 在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 Ming-Omni-Lite 在多项核心视频理解基准测试中取得了突破性进展。\n性能 我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：\n评测基准 Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 技术背后 Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：\n高效的时空建模器： 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。 高质量、多样化的视频-文本对齐数据： 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。 创新的训练目标与课程学习： 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。 迈向更智能的视频交互 Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理复杂、长时间、信息密集视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。\n语音理解和生成 突破图像生成新极限 相较于早先发布的 Ming-Lite-Uni 以及 Ming-Omni，在此版本中，我们进一步提升了Ming对生成图像的 场景一致性（Scene Consistency）、ID 一致性（Character / Style Consistency）、以及 多感知扩展（Segmentation, Keypoints, Depth, …），让Ming从一个具备图像生成和图像编辑能力的多模态大模型（MLLM），变成一个能够处理更多图像生成任务以及效果更好的完整MLLM。下面是我们最近一段时间的工作进展的报告，欢迎大家交流讨论。\nGenEval指标 Overall score Single Ojbect Two Objects Counting Color Position Color Attr Ours 0.86 100.00% 96.72% 76.56% 89.89% 89.75% 68.69% 模型结构回顾及改进 子模块 对应技术点 作用 关键点 跨模态桥接方式 Channel concat / Token concat / Blend - Channel concat：参数少、显存低，但语义对齐弱 - Token concat：保语义结构，适合大分辨率 - Blend：编辑/重绘场景更鲁棒 根据任务需求选择桥接方式。目前采用Token Concat方案 双分支表示解耦 解耦参考图图像patch编码和refiner参数 - 提升参考图的独立控制参数容量，起到部分解耦的作用 双 patchfy 模块与双分支额外 refiner，提升了模型编辑与分割性能 双分支解耦指在将图像送入 DiT 的 transformer 之前，使用不同的网络权重将参考图像与噪声图像进行 patchfy，这样能够有效降低参考图像信息对于编辑时语义遵循的影响，refiner 是在 patchfy 之后额外的两层轻量级 transformer，能够进一步增强这一效果，在推理分割上的性能评估表明了新增模块的有效性 推理分割考验模型对于语义的正确理解，需要模型根据复杂的指令确定要分割的目标 实验结果如下方表格所示，可以看到解耦的patchfy显著增强了推理分割的指标，增加 refiner 模块后能够进一步提升性能 GEdit 子集：[“background_change”, “color_alter”, “material_alter”, “motion_change”] Mode ID double-patchfy add-refiner refcoc 分割指标 GEdit(subset-full) 0 ❌ ❌ 62.8 6.129 1 ✅ ❌ 64.2 6.391 2 ✅ ✅ 64.5 6.306 条件控制与引导策略 子模块 对应技术点 作用 关键点 多条件的CFG控制策略 语义CFG vs 图像CFG（Ref-Guided） 多条件的 Classifier-free Guidance 策略：语义二分差分+图像三分差分提升ID一致性 当纯语义控制时，编辑后的图像遵循了指令，但完全丧失与原图的一致性；图像分支指导强度较大时编辑结果几乎与原图一致 ID \u0026 Scene Consistency Loss Weight mask Loss + Scene Consistency Loss 增大目标图编辑区域的权重，同时增加参考图非编辑区域的强约束和编辑区域的弱约束 调整λ平衡身份保持下的编辑效果与场景一致性，避免过拟合 与 Qwen-VLo 对比 prompt ours Qwen-VLo Make the person in the image smile slightly without altering the original structure 感知能力扩展 生成式分割\n相比于生成式图像编辑任务，分割任务的预测mask和原图之间存在较少的细节一致性，因此在token concat方案下难以较快的学习到原图和分割mask 之间的一致性关系。因此，我们将图像的分割目标建模成彩色分割图像，即mask和图像的融合形式，从而使得预测目标和原图之间存在较多一致的细节，因此能够更好的学习到分割图和原图之间的一致关系。在推理时，将预测图像和原图做diff并进行噪声过滤获取最终的预测mask。\n输入图像 推理分割 语义分割 全景分割 prompt: Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image. prompt: Please segment different classes in this image prompt: Please segment different instances in this image. 边缘轮廓图生成 原图 深度图 检测框 边缘轮廓 数据优化 ","wordCount":"284","inLanguage":"en","datePublished":"2025-07-14T00:00:03+08:00","dateModified":"2025-07-14T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://Biao-Gong.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://Biao-Gong.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Lite-Omni V1.5</h1><div class=post-meta><span title='2025-07-14 00:00:03 +0800 +0800'>July 14, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;284 words&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><h1 id=英文版待更新>【英文版待更新】<a hidden class=anchor aria-hidden=true href=#英文版待更新>#</a></h1><a href=https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify class="btn external" target=_blank>GITHUB</a><h2 id=ming-lite-omni-v15-整体评测结果>Ming-Lite-Omni V1.5 整体评测结果<a hidden class=anchor aria-hidden=true href=#ming-lite-omni-v15-整体评测结果>#</a></h2><hr><h2 id=复杂文档理解>复杂文档理解<a hidden class=anchor aria-hidden=true href=#复杂文档理解>#</a></h2><hr><h2 id=图文及体验>图文及体验<a hidden class=anchor aria-hidden=true href=#图文及体验>#</a></h2><hr><h2 id=定义视频理解新标杆>定义视频理解新标杆<a hidden class=anchor aria-hidden=true href=#定义视频理解新标杆>#</a></h2><p>在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 <strong>Ming-Omni-Lite</strong> 在多项核心视频理解基准测试中取得了突破性进展。</p><h3 id=性能>性能<a hidden class=anchor aria-hidden=true href=#性能>#</a></h3><p>我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：</p><table><thead><tr><th style=text-align:left>评测基准</th><th style=text-align:center>Qwen2.5-VL-7B</th><th style=text-align:center>Qwen2.5-Omni-7B</th><th style=text-align:center>InternVL3-8B</th><th style=text-align:center><strong>Ming-Omni-Lite</strong></th></tr></thead><tbody><tr><td style=text-align:left><strong>VideoMME(w/o subs)</strong></td><td style=text-align:center>65.10</td><td style=text-align:center>64.30</td><td style=text-align:center>66.30</td><td style=text-align:center><strong>67.07</strong></td></tr><tr><td style=text-align:left><strong>VideoMME(w/ subs)</strong></td><td style=text-align:center>71.60</td><td style=text-align:center>72.40</td><td style=text-align:center>68.90</td><td style=text-align:center><strong>72.59</strong></td></tr><tr><td style=text-align:left><strong>VideoMME(avg)</strong></td><td style=text-align:center>68.35</td><td style=text-align:center>68.35</td><td style=text-align:center>67.60</td><td style=text-align:center><strong>69.83</strong></td></tr><tr><td style=text-align:left><strong>MVBench</strong></td><td style=text-align:center>69.60</td><td style=text-align:center>70.30</td><td style=text-align:center><strong>75.40</strong></td><td style=text-align:center>69.43</td></tr><tr><td style=text-align:left><strong>LongVideoBench</strong></td><td style=text-align:center>56.00</td><td style=text-align:center>54.82</td><td style=text-align:center>58.80</td><td style=text-align:center><strong>59.54</strong></td></tr><tr><td style=text-align:left><strong>OvOBench</strong></td><td style=text-align:center>51.10</td><td style=text-align:center>50.46</td><td style=text-align:center>51.91</td><td style=text-align:center><strong>52.17</strong></td></tr></tbody></table><h3 id=技术背后>技术背后<a hidden class=anchor aria-hidden=true href=#技术背后>#</a></h3><p>Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：</p><ul><li><strong>高效的时空建模器：</strong> 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。</li><li><strong>高质量、多样化的视频-文本对齐数据：</strong> 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。</li><li><strong>创新的训练目标与课程学习：</strong> 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。</li></ul><h3 id=迈向更智能的视频交互>迈向更智能的视频交互<a hidden class=anchor aria-hidden=true href=#迈向更智能的视频交互>#</a></h3><p>Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理<strong>复杂、长时间、信息密集</strong>视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。</p><hr><h2 id=语音理解和生成>语音理解和生成<a hidden class=anchor aria-hidden=true href=#语音理解和生成>#</a></h2><hr><h2 id=突破图像生成新极限>突破图像生成新极限<a hidden class=anchor aria-hidden=true href=#突破图像生成新极限>#</a></h2><p>相较于早先发布的 Ming-Lite-Uni 以及 Ming-Omni，在此版本中，我们进一步提升了Ming对生成图像的 <strong>场景一致性</strong>（Scene Consistency）、<strong>ID 一致性</strong>（Character / Style Consistency）、以及 <strong>多感知扩展</strong>（Segmentation, Keypoints, Depth, …），让Ming从一个具备图像生成和图像编辑能力的多模态大模型（MLLM），变成一个能够处理更多图像生成任务以及效果更好的完整MLLM。下面是我们最近一段时间的工作进展的报告，欢迎大家交流讨论。</p><p><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752039359523-ef57c4ba-3f99-4a9a-9515-5728b6c46c1c.webp alt></p><table><thead><tr><th><strong>GenEval指标</strong></th><th><strong>Overall score</strong></th><th><strong>Single Ojbect</strong></th><th><strong>Two Objects</strong></th><th><strong>Counting</strong></th><th><strong>Color</strong></th><th><strong>Position</strong></th><th><strong>Color Attr</strong></th></tr></thead><tbody><tr><td>Ours</td><td>0.86</td><td>100.00%</td><td>96.72%</td><td>76.56%</td><td>89.89%</td><td>89.75%</td><td>68.69%</td></tr></tbody></table><h3 id=模型结构回顾及改进>模型结构回顾及改进<a hidden class=anchor aria-hidden=true href=#模型结构回顾及改进>#</a></h3><table><thead><tr><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">子模块</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">对应技术点</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">作用</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">关键点</font></strong></th></tr></thead><tbody><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">跨模态桥接方式</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Channel concat / Token concat / Blend</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Channel concat</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">：参数少、显存低，但语义对齐弱 </font><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Token concat</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">：保语义结构，适合大分辨率 </font><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Blend</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">：编辑/重绘场景更鲁棒</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">根据任务需求选择桥接方式。目前采用Token Concat方案</font></td></tr><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">双分支表示解耦</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">解耦参考图图像patch编码和refiner参数</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- 提升参考图的独立控制参数容量，起到部分解耦的作用</font></td><td>双 patchfy 模块与双分支额外 refiner，提升了模型编辑与分割性能</td></tr></tbody></table><ul><li>双分支解耦指在将图像送入 DiT 的 transformer 之前，使用不同的网络权重将参考图像与噪声图像进行 patchfy，这样能够有效降低参考图像信息对于编辑时语义遵循的影响，refiner 是在 patchfy 之后额外的两层轻量级 transformer，能够进一步增强这一效果，在推理分割上的性能评估表明了新增模块的有效性<ul><li>推理分割考验模型对于语义的正确理解，需要模型根据复杂的指令确定要分割的目标</li><li>实验结果如下方表格所示，可以看到解耦的patchfy显著增强了推理分割的指标，增加 refiner 模块后能够进一步提升性能</li></ul></li><li>GEdit 子集：[&ldquo;background_change&rdquo;, &ldquo;color_alter&rdquo;, &ldquo;material_alter&rdquo;, &ldquo;motion_change&rdquo;]</li></ul><table><thead><tr><th>Mode ID</th><th>double-patchfy</th><th>add-refiner</th><th>refcoc 分割指标</th><th>GEdit(subset-full)</th></tr></thead><tbody><tr><td>0</td><td>❌</td><td>❌</td><td>62.8</td><td>6.129</td></tr><tr><td>1</td><td>✅</td><td>❌</td><td>64.2</td><td>6.391</td></tr><tr><td>2</td><td>✅</td><td>✅</td><td>64.5</td><td>6.306</td></tr></tbody></table><h3 id=条件控制与引导策略>条件控制与引导策略<a hidden class=anchor aria-hidden=true href=#条件控制与引导策略>#</a></h3><table><thead><tr><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">子模块</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">对应技术点</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">作用</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">关键点</font></strong></th></tr></thead><tbody><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">多条件的CFG控制策略</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">语义CFG vs 图像CFG（Ref-Guided）</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">多条件的 Classifier-free Guidance 策略：语义二分差分+图像三分差分提升ID一致性</font></td><td>当纯语义控制时，编辑后的图像遵循了指令，但完全丧失与原图的一致性；图像分支指导强度较大时编辑结果几乎与原图一致</td></tr><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">ID & Scene Consistency Loss</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Weight mask Loss + Scene Consistency Loss</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">增大目标图编辑区域的权重，同时增加参考图非编辑区域的强约束和编辑区域的弱约束</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">调整λ平衡身份保持下的编辑效果与场景一致性，避免过拟合</font></td></tr></tbody></table><ul><li>与 Qwen-VLo 对比</li></ul><table><thead><tr><th>prompt</th><th>ours</th><th>Qwen-VLo</th></tr></thead><tbody><tr><td><font style="color:rgb(44, 44, 54);">Make the person in the image smile slightly without altering the original structure</font><br><img loading=lazy src="https://github.com/Biao-Gong/static/blob/main/gen/1752147843685-5b097f6b-b2aa-4baf-abe4-f1abd89265e8.png?raw=true" alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752147837185-62077f0c-e7ec-415f-bd34-1c8453253949.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752147953713-703c31c8-2fd1-4c2d-b4bc-6e0f52e70017.webp alt></td></tr></tbody></table><h3 id=感知能力扩展>感知能力扩展<a hidden class=anchor aria-hidden=true href=#感知能力扩展>#</a></h3><ul><li><p><strong>生成式分割</strong></p><p>相比于生成式图像编辑任务，分割任务的预测mask和原图之间存在较少的细节一致性，因此在token concat方案下难以较快的学习到原图和分割mask 之间的一致性关系。因此，我们将图像的分割目标建模成彩色分割图像，即mask和图像的融合形式，从而使得预测目标和原图之间存在较多一致的细节，因此能够更好的学习到分割图和原图之间的一致关系。在推理时，将预测图像和原图做diff并进行噪声过滤获取最终的预测mask。</p></li></ul><table><thead><tr><th>输入图像</th><th>推理分割</th><th>语义分割</th><th>全景分割</th></tr></thead><tbody><tr><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115158022-12254e69-e8c0-43fb-a725-f6730cda22d8.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115142775-3975827c-4110-445b-af53-e20201d1043a.webp alt><br>prompt: Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image.</td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752116495974-7708ba3a-5909-46df-82f5-a1bfa1519d4d.webp alt><br>prompt: Please segment different <strong>classes</strong> in this image</td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115151406-c4780a97-5f1c-46cd-9a45-d4ef600d0897.webp alt><br>prompt: Please segment different <strong>instances</strong> in this image.</td></tr></tbody></table><ul><li><strong>边缘轮廓图生成</strong></li></ul><table><thead><tr><th>原图</th><th>深度图</th><th>检测框</th><th>边缘轮廓</th></tr></thead><tbody><tr><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466889319-bd19acce-c07d-4664-9890-41e4dff1ba8d.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466903529-996bcd35-a9a0-484b-98bf-2f2468f4df42.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466895795-1955ead5-6d94-4142-8d7b-e265352d2bcb.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752467020122-ad8b436c-bb33-4ef0-85b8-cf45ae8c9be1.webp alt></td></tr></tbody></table><hr><h2 id=数据优化>数据优化<a hidden class=anchor aria-hidden=true href=#数据优化>#</a></h2></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://Biao-Gong.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>