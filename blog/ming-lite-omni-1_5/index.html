<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing Ming-Lite-Omni V1.5 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on 🤗Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://Biao-Gong.github.io/favicon.png><link rel=apple-touch-icon href=https://Biao-Gong.github.io/favicon.png><link rel=manifest href=https://Biao-Gong.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Introducing Ming-Lite-Omni V1.5"><meta property="og:description" content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on 🤗Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><meta property="og:type" content="article"><meta property="og:url" content="https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-18T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-18T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Introducing Ming-Lite-Omni V1.5"><meta name=twitter:description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on 🤗Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://Biao-Gong.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Introducing Ming-Lite-Omni V1.5","item":"https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Ming-Lite-Omni V1.5","name":"Introducing Ming-Lite-Omni V1.5","description":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\nWe are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on 🤗Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.\nBuilt on the Ling-lite-1.5 architecture, Ming-lite-omni V1.","keywords":[],"articleBody":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\nWe are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on 🤗Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.\nBuilt on the Ling-lite-1.5 architecture, Ming-lite-omni V1.5 features a total of 20.3B parameters, with 3B active parameters in its Mixture of Experts (MoE) configuration. It has achieved impressive results across various multimodal benchmarks. Below, we highlight the key advancements in performance metrics and model architecture from this latest update.\nPerformance Comparison\nFramework\nIntroduction To achieve these advancements, we integrated our in-house innovations with cutting-edge developments from both academia and the open-source community. We explored several key areas, leading to significant breakthroughs and key findings detailed below.\nImage and Speech Generation Dual-Branch Decoupling for Image Generation: We implemented a dual-branch decoupling strategy that processes the reference image and the noise image with separate weights before they enter the Diffusion Transformer (DiT). This approach effectively increases the model’s capacity for learning from the reference. To further refine the output, we added two extra transformer layers. ID \u0026 Scene Consistency Loss: To maintain character and scene identity during image editing, we introduced a new ID \u0026 Scene Consistency Loss. This function strategically increases focus on the edited region in the target image, boosts the influence of unedited areas from the reference image, and diminishes the influence of the edited areas from the reference. Perceptual Enhancement Strategy: We introduced a Perceptual Enhancement Strategy by improving the model’s ability to perceive structures through tasks like segmentation and keypoint detection. This provides greater structural control during generation and editing, leading to significant score improvements on metrics related to object position, structure, and count (see Table A for details). Multi-task Collaborative Learning: A Multi-task Collaborative Learning approach allows generation and editing tasks to synergistically improve one another. By treating segmentation as a colorization-based editing task within a unified training pipeline, we achieved significant gains in segmentation accuracy and enhanced the precision of local edits, resulting in smoother and more natural transitions at the edges of edited areas. New Audio Decoder: On the speech generation front, we developed a novel audio decoder that is fully context-aware, directly processing output features from the LLM. Efficient Speech Generation: To improve prosody and real-time generation capabilities, we applied BPE encoding to the discrete audio codec tokens, which successfully reduced the audio frame rate by 35%. Comprehensive Data Upgrades: Sourced high-quality portrait data, filtered by resolution, facial detail, and face size. Collected and curated a high-quality celebrity dataset with processed face crops. Expanded the model’s capabilities by building new training subsets for edge maps, segmentation maps, text-in-image, and facial expressions. Image, Text, Video, and Speech Understanding MRoPE for Spatiotemporal Awareness: We integrated MRoPE (Multi-dimensional RoPE), which applies positional encoding across three dimensions (time, height, and width). This equips the model with a native understanding of space and time, enabling highly efficient cross-modal modeling and significantly boosting comprehension accuracy for videos and complex images. Efficient Full-Parameter Training: By optimizing learning rates and the multimodal data mixture, we upgraded our pre-training from a phased approach of freezing/unfreezing LLM layers to a streamlined, efficient full-parameter training regimen. This shortened the training cycle by 26.5% with no loss in performance. Video Understanding via Curriculum Learning: We adopted a curriculum learning strategy for video understanding, progressively increasing the complexity from short-form to long-form videos to enhance the model’s capabilities. Complex Document Understanding with CoT: For complex document analysis, we incorporated a Chain-of-Thought (CoT) strategy to build structured reasoning paths, effectively improving the model’s problem-solving abilities on intricate queries. Comprehensive Data Upgrades: Pre-training: Added structured text entity data to fill knowledge graph gaps. Expanded with high-quality product data to enhance general knowledge. Instruction Fine-Tuning: Improved data precision for fine-grained visual perception (e.g., object counting, color, and scene recognition). Increased data depth for specialized domain recognition (e.g., flora/fauna, vehicles, and ingredients). Enhanced cross-disciplinary reasoning in complex visual-text tasks through targeted data optimization. For speech understanding, we infused instruction text with metadata like domain, topic, and language (including dialects). This boosted model performance and enabled comprehensive support for Mandarin, English, Cantonese, Sichuanese, Shanghainese, and Minnan. Aligning with User Preferences To ensure that improvements on standard benchmarks translate directly to a superior real-world user experience, we created our own human evaluation benchmark and conducted internal, head-to-head evaluations against other models. Thanks to our high-quality preference alignment data, Ming-lite-omni V1.5 demonstrates a distinct advantage over leading models in the accuracy (low hallucination rate), relevance, formatting, and fluency of its visual question-answering responses. On our internal adversarial test set, Ming-lite-omni V1.5 achieved an 87.07% win rate against Ming-lite-omni V1, marking a significant enhancement in user experience.\nBenchmark Dimension Qwen2.5-VL-7B Ming-Omni-Lite V1.5 Relevance 4.308 4.5 Fluency 4.765 4.91 Richness 3.828 3.69 Formatting 4.727 4.8 Accuracy 3.741 3.92 Average 4.274 4.365 Demos Image Editing To address the challenge of maintaining character and scene identity during image editing, we’ve introduced the ID \u0026 Scene Consistency Loss. This function strategically increases the weight of the edited region in the target image, boosts the influence of unedited areas from the reference image, and reduces the influence of the edited areas within that same reference.\nGenerative Image Segmentation Next User Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image. Ming-Lite-Omni V1.5 User Please segment different classes in this image. Ming-Lite-Omni V1.5 User Please segment different instances in this image. Ming-Lite-Omni V1.5 Image Generation We introduced a Perceptual Enhancement Strategy by optimizing the model’s structural awareness through tasks like segmentation and keypoint detection. This improves its understanding of image details and spatial relationships, giving us greater structural control over the generation and editing process. The result is a significant boost in scores on the GenEval benchmark, particularly for metrics related to position, structure, and count: 1-Obj 2-Obj Counting Colors Position Color Attr Avg. Ming-lite-omni 0.99 0.77 0.68 0.78 0.46 0.42 0.64 Ming-lite-omni V1.5 0.99 0.93 0.86 0.87 0.90 0.66 0.87 Original Image Generated Depth Map Generated Bounding Boxes Generated Edge Map Visual Dialogue Next User 这张图片看起来设计感很强烈，可以详细描述一下它的各个设计元素？ Ming-Lite-Omni 这张图片展示了一款高端游戏鼠标，具有现代感和科技感的设计。鼠标主体为黑色，表面光滑且具有金属质感。鼠标顶部有一个蓝色的发光标志，标志下方是鼠标滚轮和左右按键区域。鼠标侧面有蓝色的灯光装饰，增加了视觉吸引力。鼠标底部有防滑纹理设计，确保使用时的稳定性。整体设计简洁而富有未来感，适合游戏玩家使用。 Ming-Lite-Omni V1.5 这张图片展示了一款设计感强烈的游戏鼠标，具有以下设计元素： 1. 颜色和材质： - 鼠标主体为黑色，表面光滑且具有金属质感。 - 鼠标边缘和按钮部分有蓝色LED灯带，增加了科技感和视觉吸引力。 2. 形状和结构： - 鼠标整体呈流线型设计，顶部略微倾斜，便于手部自然放置。 - 鼠标侧面有明显的曲线设计，符合人体工程学，提供舒适的握持感。 3. 按钮和滚轮： - 鼠标左侧有一个滚轮，滚轮上方有蓝色LED灯带。 - 鼠标右侧有额外的按钮，这些按钮也有蓝色LED灯带，增加了功能性和美观性。 4. 品牌标识： - 鼠标底部中央有一个发光的“G”标志，这是该品牌的标志性设计元素。 5. 灯光效果： - 鼠标上的蓝色LED灯带在暗色背景下显得尤为突出，增强了科技感和未来感。 6. 纹理和细节： - 鼠标顶部有纹理设计，可能是为了增加防滑效果。 - 鼠标侧面有细长的纹理设计，增加了视觉上的层次感。 整体来看，这款鼠标的设计结合了现代科技感和人体工程学设计，既美观又实用，适合游戏玩家使用。 User 图中的书法为什么好？ Ming-Lite-Omni 这幅书法作品展示了高超的书法技巧和深厚的文化底蕴。笔法流畅自然，字形结构严谨，布局合理，体现了书法家的深厚功力和艺术修养。 Ming-Lite-Omni V1.5 这幅书法作品展示了几个显著的特点，使其成为一件优秀的书法作品： 1. 笔法精湛：书法的笔法流畅自然，线条粗细变化丰富，表现出书法家的深厚功力和熟练技巧。 2. 结构严谨：每个字的结构都经过精心设计，笔画之间的衔接自然，字与字之间保持了良好的平衡和协调。 3. 章法布局：整幅作品的章法布局合理，行距和字距均匀，给人一种整齐有序的感觉。 4. 墨色变化：墨色的浓淡变化自然，增添了作品的层次感和立体感。 5. 内容优美：书法内容是一篇优美的古文，文字内容富有哲理和文学价值，增加了作品的文化内涵。 6. 印章点缀：作品上有红色的印章，印章的使用不仅增加了作品的装饰性，还起到了点缀和平衡的作用。 这些特点共同构成了这幅书法作品的高超艺术价值，使其成为一件值得欣赏和学习的佳作。 User 图中这款手机适合给父母买吗？ Ming-Lite-Omni 这款手机外观简约时尚，适合追求简约风格的父母使用。相机配置也较为先进，适合喜欢拍照的用户。 Ming-Lite-Omni V1.5 这款手机适合给父母购买，原因如下： 1. 外观设计：手机采用简约时尚的设计，白色机身显得干净利落，符合现代审美。 2. 摄像头配置：后置摄像头设计独特，排列整齐，可能提供良好的拍照体验。 3. 品牌：荣耀（Honor）品牌在智能手机市场上有良好的口碑，提供稳定可靠的产品。 4. 功能：通常这类手机会配备大容量电池和长续航能力，适合日常使用。 5. 易用性：可能提供简洁的用户界面和易于操作的功能，适合不太熟悉复杂操作的用户。 6. 性价比：荣耀手机通常以高性价比著称，适合预算有限但希望获得良好体验的用户。 当然，具体是否适合还需考虑父母的实际需求和偏好。 User 如何使用图中软件购买机票？请列出具体操作步骤 Ming-Lite-Omni 使用图中软件购买机票的具体操作步骤如下： 1. 打开携程app。 2. 点击“机票”选项。 3. 在机票页面选择出发地、目的地、出发日期和返回日期。 4. 点击搜索按钮，查看可用的航班信息。 5. 选择合适的航班后，点击“预订”按钮。 6. 填写乘客信息并选择支付方式。 7. 确认订单信息并提交支付，完成购票。 Ming-Lite-Omni V1.5 使用携程软件购买机票的具体操作步骤如下： 1. 下载并安装携程App： - 如果您还没有安装携程App，可以在应用商店（如苹果App Store或安卓应用市场）搜索“携程旅行”并下载安装。 2. 打开携程App： - 安装完成后，打开携程App。 3. 选择“机票”选项： - 在首页的导航栏中，点击“机票”选项。 4. 输入出发地和目的地： - 在机票预订页面，输入您的出发地和目的地。 5. 选择出发日期和返回日期： - 选择您的出发日期和返回日期（如果是往返票）。 6. 选择乘客信息： - 输入乘客人数和座位类型（成人、儿童、婴儿）。 7. 选择航班： - 浏览并选择您喜欢的航班。 8. 填写乘客信息： - 输入乘客的姓名、身份证号码、联系方式等信息。 9. 选择支付方式： - 选择您喜欢的支付方式（如支付宝、微信支付、银行卡等）。 10. 确认订单并支付： - 确认订单信息无误后，点击“支付”按钮完成支付。 11. 接收电子机票： - 支付成功后，您将收到电子机票，可以通过App内的“我的订单”查看或下载。 通过以上步骤，您就可以使用携程App轻松购买机票了。 User 根据图表计算样本之间的 Δct 在哪一个引物下相差最小？ | 根据图表计算样本之间的 Δct 在 cGAS-qF+qR 引物下相差最小。（图片来源：互联网公开-舒桐科技公众号） Ming-Lite-Omni 根据图表计算样本之间的 Δct 在 cGAS-qF+qR 引物下相差最小。 Ming-Lite-Omni V1.5 在 EGFP-cGAS单克隆 引物下，样本之间的 Δct 相差最小。 计算过程： 1. 理解 Δct 的含义： Δct 是目标基因（cGAS-qF+qR 或 EGFP-cGAS）与内参基因（cTubulin）的相对表达量差值。Δct 越小，说明目标基因与内参基因的表达量差异越小。 2. 比较不同引物下的 Δct： - cGAS-qF+qR 引物： - 野生型：Δct = 8.06 - EGFP-cGAS单克隆：Δct = -0.15 - 差异：8.06 - (-0.15) = 8.21 - EGFP-cGAS单克隆 引物： - 野生型：Δct = 10.44 - EGFP-cGAS单克隆：Δct = -2.9 - 差异：10.44 - (-2.9) = 13.34 - EGFP-cGAS-qF+qR 引物： - 野生型：Δct = 21.43 - EGFP-cGAS单克隆：Δct = -13.45 - 差异：21.43 - (-13.45) = 34.88 3. 结论： 在 EGFP-cGAS单克隆 引物下，样本之间的 Δct 差异最小（Δct 差异为 13.34），说明该引物下样本之间的表达量差异最小。 总结： 在 EGFP-cGAS单克隆 引物下，样本之间的 Δct 相差最小。 Speech Generation Text Inputs Ming-lite-omni Ming-lite-omni V1.5 Tyler, Lucy, Michelle, we’re going to space! what is the forecast for California for rain Television can make you dumb, but it can also be good education. OCR Next User 在判断题第95题中，考生答题的手写内容是什么？若考生的手写答案为√则输出对，若考生的手写答案为×则输出错，请输出对或错 Ming-Lite-Omni 对 Ming-Lite-Omni V1.5 错 User Output the content of the CAPTCHA directly. Characters are only numbers and uppercase/lowercase letters. Ming-Lite-Omni 9743 Ming-Lite-Omni V1.5 9742 User What is the Latex tag for mathematical expression in images? Ming-Lite-Omni A C = C B \\therefore C D / / B E Ming-Lite-Omni V1.5 A C = C B \\because C D / / B E User 从图中提取: 日期 Ming-Lite-Omni 日期：2019年12月03日05时08分 从图中提取: 里程 里程：23.9公里 从图中提取: 金额 金额：52.6元 从图中提取: 状态 状态： 从图中提取: 客票印务 客票印务2019年12月印900000卷 从图中提取: 号码 号码从00000001至90000000止 Ming-Lite-Omni V1.5 日期: 2021年03月05日 User What is the difference in value between Lamb and Corn?’ You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within [think] [/think] tags. The final answer MUST BE put in \\boxed{}.' Ming-Lite-Omni 0.37 Ming-Lite-Omni V1.5 [think]The value for Lamb is 103.7 and for Corn is 103.13. To find the difference, subtract the smaller value from the larger value: 103.7 - 103.13 = 0.57.[/think]\\boxed{0.57} Get Started with Ming-lite-omni v1.5 The model and code for Ming-lite-omni v1.5 are now open-source, and we invite everyone to try it out, share feedback, and join the discussion. Looking ahead, we’re excited to announce that a quantized and accelerated version is on the way. This future release will not only further enhance omni-modal performance but also make the model even more lightweight, all while strengthening its multimodal reasoning and generation capabilities. Stay tuned for more updates!\nGithub: https://github.com/inclusionAI/Ming Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5 ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5 ","wordCount":"1578","inLanguage":"en","datePublished":"2025-07-18T00:00:03+08:00","dateModified":"2025-07-18T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://Biao-Gong.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://Biao-Gong.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Introducing Ming-Lite-Omni V1.5</h1><div class=post-meta><span title='2025-07-18 00:00:03 +0800 +0800'>July 18, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1578 words&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> 🤗 <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>Hugging Face</a>｜ 🤖 <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>ModelScope</a></p><p>We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on 🤗<a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni>Hugging Face</a>). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.</p><p>Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1.5 features a total of 20.3B parameters, with 3B active parameters in its Mixture of Experts (MoE) configuration. It has achieved impressive results across various multimodal benchmarks. Below, we highlight the key advancements in performance metrics and model architecture from this latest update.</p><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/bYseTozopmYAAAAAblAAAAgAeuUHAQFr/fmt.webp alt="Image description"><p style=font-size:14px;color:gray>Performance Comparison</p></div><div style=text-align:center><p style=font-size:14px;color:gray>Framework</p></div><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>To achieve these advancements, we integrated our in-house innovations with cutting-edge developments from both academia and the open-source community. We explored several key areas, leading to significant breakthroughs and key findings detailed below.</p><h3 id=image-and-speech-generation>Image and Speech Generation<a hidden class=anchor aria-hidden=true href=#image-and-speech-generation>#</a></h3><ol><li><strong>Dual-Branch Decoupling for Image Generation:</strong> We implemented a <strong>dual-branch decoupling strategy</strong> that processes the reference image and the noise image with separate weights before they enter the Diffusion Transformer (DiT). This approach effectively increases the model&rsquo;s capacity for learning from the reference. To further refine the output, we added two extra transformer layers.</li><li><strong>ID & Scene Consistency Loss:</strong> To maintain character and scene identity during image editing, we introduced a new <strong>ID & Scene Consistency Loss</strong>. This function strategically increases focus on the edited region in the target image, boosts the influence of unedited areas from the reference image, and diminishes the influence of the edited areas from the reference.</li><li><strong>Perceptual Enhancement Strategy:</strong> We introduced a <strong>Perceptual Enhancement Strategy</strong> by improving the model&rsquo;s ability to perceive structures through tasks like segmentation and keypoint detection. This provides greater structural control during generation and editing, leading to significant score improvements on metrics related to object position, structure, and count (see <a href=#table1>Table A</a> for details).</li><li><strong>Multi-task Collaborative Learning:</strong> A <strong>Multi-task Collaborative Learning</strong> approach allows generation and editing tasks to synergistically improve one another. By treating segmentation as a colorization-based editing task within a unified training pipeline, we achieved significant gains in segmentation accuracy and enhanced the precision of local edits, resulting in smoother and more natural transitions at the edges of edited areas.</li><li><strong>New Audio Decoder:</strong> On the speech generation front, we developed a novel audio decoder that is fully context-aware, directly processing output features from the LLM.</li><li><strong>Efficient Speech Generation:</strong> To improve prosody and real-time generation capabilities, we applied BPE encoding to the discrete audio codec tokens, which successfully reduced the audio frame rate by 35%.</li><li><strong>Comprehensive Data Upgrades:</strong><ul><li>Sourced high-quality portrait data, filtered by resolution, facial detail, and face size.</li><li>Collected and curated a high-quality celebrity dataset with processed face crops.</li><li>Expanded the model&rsquo;s capabilities by building new training subsets for edge maps, segmentation maps, text-in-image, and facial expressions.</li></ul></li></ol><h3 id=image-text-video-and-speech-understanding>Image, Text, Video, and Speech Understanding<a hidden class=anchor aria-hidden=true href=#image-text-video-and-speech-understanding>#</a></h3><ol><li><strong>MRoPE for Spatiotemporal Awareness:</strong> We integrated MRoPE (Multi-dimensional RoPE), which applies positional encoding across three dimensions (time, height, and width). This equips the model with a native understanding of space and time, enabling highly efficient cross-modal modeling and significantly boosting comprehension accuracy for videos and complex images.</li><li><strong>Efficient Full-Parameter Training:</strong> By optimizing learning rates and the multimodal data mixture, we upgraded our pre-training from a phased approach of <strong>freezing/unfreezing LLM layers</strong> to a streamlined, <strong>efficient full-parameter training</strong> regimen. This shortened the training cycle by 26.5% with no loss in performance.</li><li><strong>Video Understanding via Curriculum Learning:</strong> We adopted a curriculum learning strategy for video understanding, progressively increasing the complexity from short-form to long-form videos to enhance the model&rsquo;s capabilities.</li><li><strong>Complex Document Understanding with CoT:</strong> For complex document analysis, we incorporated a Chain-of-Thought (CoT) strategy to build structured reasoning paths, effectively improving the model&rsquo;s problem-solving abilities on intricate queries.</li><li><strong>Comprehensive Data Upgrades:</strong><ul><li><strong>Pre-training:</strong><ul><li>Added structured text entity data to fill knowledge graph gaps.</li><li>Expanded with high-quality product data to enhance general knowledge.</li></ul></li><li><strong>Instruction Fine-Tuning:</strong><ul><li>Improved data precision for fine-grained visual perception (e.g., object counting, color, and scene recognition).</li><li>Increased data depth for specialized domain recognition (e.g., flora/fauna, vehicles, and ingredients).</li><li>Enhanced cross-disciplinary reasoning in complex visual-text tasks through targeted data optimization.</li><li>For speech understanding, we infused instruction text with metadata like domain, topic, and language (including dialects). This boosted model performance and enabled comprehensive support for Mandarin, English, Cantonese, Sichuanese, Shanghainese, and Minnan.</li></ul></li></ul></li></ol><h3 id=aligning-with-user-preferences>Aligning with User Preferences<a hidden class=anchor aria-hidden=true href=#aligning-with-user-preferences>#</a></h3><p>To ensure that improvements on standard benchmarks translate directly to a superior real-world user experience, we created our own <strong>human evaluation benchmark</strong> and conducted internal, head-to-head evaluations against other models. Thanks to our high-quality preference alignment data, Ming-lite-omni V1.5 demonstrates a distinct advantage over leading models in the accuracy (low hallucination rate), relevance, formatting, and fluency of its visual question-answering responses. On our internal adversarial test set, Ming-lite-omni V1.5 achieved an <strong>87.07% win rate</strong> against Ming-lite-omni V1, marking a significant enhancement in user experience.</p><div style=text-align:center;margin:auto;width:80%;line-height:1.1><table style="margin:0 auto"><thead><tr><th style=text-align:center;padding:5px>Benchmark Dimension</th><th style=text-align:center;padding:5px>Qwen2.5-VL-7B</th><th style=text-align:center;padding:5px>Ming-Omni-Lite V1.5</th></tr></thead><tbody><tr><td style=text-align:center;padding:2px>Relevance</td><td style=text-align:center;padding:2px>4.308</td><td style=text-align:center;padding:2px>4.5</td></tr><tr><td style=text-align:center;padding:2px>Fluency</td><td style=text-align:center;padding:2px>4.765</td><td style=text-align:center;padding:2px>4.91</td></tr><tr><td style=text-align:center;padding:2px>Richness</td><td style=text-align:center;padding:2px>3.828</td><td style=text-align:center;padding:2px>3.69</td></tr><tr><td style=text-align:center;padding:2px>Formatting</td><td style=text-align:center;padding:2px>4.727</td><td style=text-align:center;padding:2px>4.8</td></tr><tr><td style=text-align:center;padding:2px>Accuracy</td><td style=text-align:center;padding:2px>3.741</td><td style=text-align:center;padding:2px>3.92</td></tr><tr><td style=text-align:center;padding:2px><strong>Average</strong></td><td style=text-align:center;padding:2px><strong>4.274</strong></td><td style=text-align:center;padding:2px><strong>4.365</strong></td></tr></tbody></table></div><br><h2 id=demos>Demos<a hidden class=anchor aria-hidden=true href=#demos>#</a></h2><h3 id=image-editing>Image Editing<a hidden class=anchor aria-hidden=true href=#image-editing>#</a></h3><p>To address the challenge of maintaining character and scene identity during image editing, we&rsquo;ve introduced the <strong>ID & Scene Consistency Loss</strong>. This function strategically increases the weight of the edited region in the target image, boosts the influence of unedited areas from the reference image, and reduces the influence of the edited areas within that same reference.</p><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/e-mDS5UyUogAAAAAgCAAAAgAeuUHAQFr/fmt.webp alt="Image description">
<video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/UoqbRYQnZYEAAAAAgCAAAAgAeuUHAQFr controls></video></div><div class="full-width-container example-container"><div class=example-content><div class=title><span>Generative Image Segmentation</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115158022-12254e69-e8c0-43fb-a725-f6730cda22d8.webp height=256px></figure>Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115142775-3975827c-4110-445b-af53-e20201d1043a.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different classes in this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752116495974-7708ba3a-5909-46df-82f5-a1bfa1519d4d.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different instances in this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115151406-c4780a97-5f1c-46cd-9a45-d4ef600d0897.webp height=256px></figure></div></div></div></div><h3 id=image-generation>Image Generation<a hidden class=anchor aria-hidden=true href=#image-generation>#</a></h3><p>We introduced a <strong>Perceptual Enhancement Strategy</strong> by optimizing the model&rsquo;s structural awareness through tasks like segmentation and keypoint detection. This improves its understanding of image details and spatial relationships, giving us greater structural control over the generation and editing process. The result is a significant boost in scores on the GenEval benchmark, particularly for metrics related to position, structure, and count:
<a id=table1></a></p><table><thead><tr><th></th><th style=text-align:center>1-Obj</th><th style=text-align:center>2-Obj</th><th style=text-align:center>Counting</th><th style=text-align:center>Colors</th><th style=text-align:center>Position</th><th style=text-align:center>Color Attr</th><th style=text-align:center>Avg.</th></tr></thead><tbody><tr><td>Ming-lite-omni</td><td style=text-align:center>0.99</td><td style=text-align:center>0.77</td><td style=text-align:center>0.68</td><td style=text-align:center>0.78</td><td style=text-align:center>0.46</td><td style=text-align:center>0.42</td><td style=text-align:center>0.64</td></tr><tr><td>Ming-lite-omni V1.5</td><td style=text-align:center>0.99</td><td style=text-align:center>0.93</td><td style=text-align:center>0.86</td><td style=text-align:center>0.87</td><td style=text-align:center>0.90</td><td style=text-align:center>0.66</td><td style=text-align:center>0.87</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Original Image</th><th style=text-align:center>Generated Depth Map</th><th style=text-align:center>Generated Bounding Boxes</th><th style=text-align:center>Generated Edge Map</th></tr></thead><tbody><tr><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466889319-bd19acce-c07d-4664-9890-41e4dff1ba8d.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466903529-996bcd35-a9a0-484b-98bf-2f2468f4df42.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466895795-1955ead5-6d94-4142-8d7b-e265352d2bcb.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752467020122-ad8b436c-bb33-4ef0-85b8-cf45ae8c9be1.webp alt></td></tr></tbody></table><div class="full-width-container example-container"><div class=example-content><div class=title><span>Visual Dialogue</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/2.jpg height=256px></figure>这张图片看起来设计感很强烈，可以详细描述一下它的各个设计元素？</div><div class=role>Ming-Lite-Omni</div><div class=content>这张图片展示了一款高端游戏鼠标，具有现代感和科技感的设计。鼠标主体为黑色，表面光滑且具有金属质感。鼠标顶部有一个蓝色的发光标志，标志下方是鼠标滚轮和左右按键区域。鼠标侧面有蓝色的灯光装饰，增加了视觉吸引力。鼠标底部有防滑纹理设计，确保使用时的稳定性。整体设计简洁而富有未来感，适合游戏玩家使用。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>这张图片展示了一款设计感强烈的游戏鼠标，具有以下设计元素：<br>1. <strong>颜色和材质</strong>： - 鼠标主体为黑色，表面光滑且具有金属质感。 - 鼠标边缘和按钮部分有蓝色LED灯带，增加了科技感和视觉吸引力。<br>2. <strong>形状和结构</strong>： - 鼠标整体呈流线型设计，顶部略微倾斜，便于手部自然放置。 - 鼠标侧面有明显的曲线设计，符合人体工程学，提供舒适的握持感。<br>3. <strong>按钮和滚轮</strong>： - 鼠标左侧有一个滚轮，滚轮上方有蓝色LED灯带。 - 鼠标右侧有额外的按钮，这些按钮也有蓝色LED灯带，增加了功能性和美观性。<br>4. <strong>品牌标识</strong>： - 鼠标底部中央有一个发光的“G”标志，这是该品牌的标志性设计元素。<br>5. <strong>灯光效果</strong>： - 鼠标上的蓝色LED灯带在暗色背景下显得尤为突出，增强了科技感和未来感。<br>6. <strong>纹理和细节</strong>： - 鼠标顶部有纹理设计，可能是为了增加防滑效果。 - 鼠标侧面有细长的纹理设计，增加了视觉上的层次感。<br>整体来看，这款鼠标的设计结合了现代科技感和人体工程学设计，既美观又实用，适合游戏玩家使用。</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/3.webp height=256px></figure>图中的书法为什么好？</div><div class=role>Ming-Lite-Omni</div><div class=content>这幅书法作品展示了高超的书法技巧和深厚的文化底蕴。笔法流畅自然，字形结构严谨，布局合理，体现了书法家的深厚功力和艺术修养。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>这幅书法作品展示了几个显著的特点，使其成为一件优秀的书法作品：<br>1. <strong>笔法精湛</strong>：书法的笔法流畅自然，线条粗细变化丰富，表现出书法家的深厚功力和熟练技巧。<br>2. <strong>结构严谨</strong>：每个字的结构都经过精心设计，笔画之间的衔接自然，字与字之间保持了良好的平衡和协调。<br>3. <strong>章法布局</strong>：整幅作品的章法布局合理，行距和字距均匀，给人一种整齐有序的感觉。<br>4. <strong>墨色变化</strong>：墨色的浓淡变化自然，增添了作品的层次感和立体感。<br>5. <strong>内容优美</strong>：书法内容是一篇优美的古文，文字内容富有哲理和文学价值，增加了作品的文化内涵。<br>6. <strong>印章点缀</strong>：作品上有红色的印章，印章的使用不仅增加了作品的装饰性，还起到了点缀和平衡的作用。<br>这些特点共同构成了这幅书法作品的高超艺术价值，使其成为一件值得欣赏和学习的佳作。</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/4.webp height=256px></figure>图中这款手机适合给父母买吗？</div><div class=role>Ming-Lite-Omni</div><div class=content>这款手机外观简约时尚，适合追求简约风格的父母使用。相机配置也较为先进，适合喜欢拍照的用户。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>这款手机适合给父母购买，原因如下：<br>1. <strong>外观设计</strong>：手机采用简约时尚的设计，白色机身显得干净利落，符合现代审美。<br>2. <strong>摄像头配置</strong>：后置摄像头设计独特，排列整齐，可能提供良好的拍照体验。<br>3. <strong>品牌</strong>：荣耀（Honor）品牌在智能手机市场上有良好的口碑，提供稳定可靠的产品。<br>4. <strong>功能</strong>：通常这类手机会配备大容量电池和长续航能力，适合日常使用。<br>5. <strong>易用性</strong>：可能提供简洁的用户界面和易于操作的功能，适合不太熟悉复杂操作的用户。<br>6. <strong>性价比</strong>：荣耀手机通常以高性价比著称，适合预算有限但希望获得良好体验的用户。<br>当然，具体是否适合还需考虑父母的实际需求和偏好。</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/6.webp height=256px></figure>如何使用图中软件购买机票？请列出具体操作步骤</div><div class=role>Ming-Lite-Omni</div><div class=content>使用图中软件购买机票的具体操作步骤如下：<br>1. 打开携程app。 2. 点击“机票”选项。 3. 在机票页面选择出发地、目的地、出发日期和返回日期。 4. 点击搜索按钮，查看可用的航班信息。 5. 选择合适的航班后，点击“预订”按钮。 6. 填写乘客信息并选择支付方式。 7. 确认订单信息并提交支付，完成购票。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>使用携程软件购买机票的具体操作步骤如下：<br>1. <strong>下载并安装携程App</strong>： - 如果您还没有安装携程App，可以在应用商店（如苹果App Store或安卓应用市场）搜索“携程旅行”并下载安装。<br>2. <strong>打开携程App</strong>： - 安装完成后，打开携程App。<br>3. <strong>选择“机票”选项</strong>： - 在首页的导航栏中，点击“机票”选项。<br>4. <strong>输入出发地和目的地</strong>： - 在机票预订页面，输入您的出发地和目的地。<br>5. <strong>选择出发日期和返回日期</strong>： - 选择您的出发日期和返回日期（如果是往返票）。<br>6. <strong>选择乘客信息</strong>： - 输入乘客人数和座位类型（成人、儿童、婴儿）。<br>7. <strong>选择航班</strong>： - 浏览并选择您喜欢的航班。<br>8. <strong>填写乘客信息</strong>： - 输入乘客的姓名、身份证号码、联系方式等信息。<br>9. <strong>选择支付方式</strong>： - 选择您喜欢的支付方式（如支付宝、微信支付、银行卡等）。<br>10. <strong>确认订单并支付</strong>： - 确认订单信息无误后，点击“支付”按钮完成支付。<br>11. <strong>接收电子机票</strong>： - 支付成功后，您将收到电子机票，可以通过App内的“我的订单”查看或下载。<br>通过以上步骤，您就可以使用携程App轻松购买机票了。</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/7.webp height=128px></figure>根据图表计算样本之间的 Δct 在哪一个引物下相差最小？ | 根据图表计算样本之间的 Δct 在 cGAS-qF+qR 引物下相差最小。（图片来源：互联网公开-舒桐科技公众号）</div><div class=role>Ming-Lite-Omni</div><div class=content>根据图表计算样本之间的 Δct 在 cGAS-qF+qR 引物下相差最小。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>在 <strong>EGFP-cGAS单克隆</strong> 引物下，样本之间的 Δct 相差最小。<br><br><strong>计算过程：</strong><br><strong>1. 理解 Δct 的含义：</strong> Δct 是目标基因（cGAS-qF+qR 或 EGFP-cGAS）与内参基因（cTubulin）的相对表达量差值。Δct 越小，说明目标基因与内参基因的表达量差异越小。<br><strong>2. 比较不同引物下的 Δct：</strong><br>- <strong>cGAS-qF+qR 引物：</strong> - 野生型：Δct = 8.06 - EGFP-cGAS单克隆：Δct = -0.15 - 差异：8.06 - (-0.15) = 8.21<br>- <strong>EGFP-cGAS单克隆 引物：</strong> - 野生型：Δct = 10.44 - EGFP-cGAS单克隆：Δct = -2.9 - 差异：10.44 - (-2.9) = 13.34<br>- <strong>EGFP-cGAS-qF+qR 引物：</strong> - 野生型：Δct = 21.43 - EGFP-cGAS单克隆：Δct = -13.45 - 差异：21.43 - (-13.45) = 34.88<br><strong>3. 结论：</strong> 在 <strong>EGFP-cGAS单克隆</strong> 引物下，样本之间的 Δct 差异最小（Δct 差异为 13.34），说明该引物下样本之间的表达量差异最小。<br><br><strong>总结：</strong> 在 <strong>EGFP-cGAS单克隆</strong> 引物下，样本之间的 Δct 相差最小。</div></div></div></div><h3 id=speech-generation>Speech Generation<a hidden class=anchor aria-hidden=true href=#speech-generation>#</a></h3><table><thead><tr><th>Text Inputs</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Ming-lite-omni V1.5</th></tr></thead><tbody><tr><td>Tyler, Lucy, Michelle, we&rsquo;re going to space!</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en1_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en1_v1.5.wav type=audio/wav></audio></td></tr><tr><td>what is the forecast for California for rain</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en2_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en2_v1.5.wav type=audio/wav></audio></td></tr><tr><td>Television can make you dumb, but it can also be good education.</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en3_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en3_v1.5.wav type=audio/wav></audio></td></tr></tbody></table><div class="full-width-container example-container"><div class=example-content><div class=title><span>OCR</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/9.webp height=320px></figure>在判断题第95题中，考生答题的手写内容是什么？若考生的手写答案为√则输出对，若考生的手写答案为×则输出错，请输出对或错</div><div class=role>Ming-Lite-Omni</div><div class=content>对</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>错</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/10.webp height=64px></figure>Output the content of the CAPTCHA directly. Characters are only numbers and uppercase/lowercase letters.</div><div class=role>Ming-Lite-Omni</div><div class=content>9743</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>9742</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/11.webp height=64px></figure>What is the Latex tag for mathematical expression in images?</div><div class=role>Ming-Lite-Omni</div><div class=content>A C = C B \therefore C D / / B E</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>A C = C B \because C D / / B E</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/12.webp height=320px></figure>从图中提取: 日期</div><div class=role>Ming-Lite-Omni</div><div class=content>日期：2019年12月03日05时08分 从图中提取: 里程 里程：23.9公里 从图中提取: 金额 金额：52.6元 从图中提取: 状态 状态： 从图中提取: 客票印务 客票印务2019年12月印900000卷 从图中提取: 号码 号码从00000001至90000000止</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>日期: 2021年03月05日</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/13.webp height=320px></figure>What is the difference in value between Lamb and Corn?&rsquo; You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within [think] [/think] tags. The final answer MUST BE put in \boxed{}.'</div><div class=role>Ming-Lite-Omni</div><div class=content>0.37</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>[think]<br>The value for Lamb is 103.7 and for Corn is 103.13. To find the difference, subtract the smaller value from the larger value: 103.7 - 103.13 = 0.57.<br>[/think]<br>\boxed{0.57}</div></div></div></div><h2 id=get-started-with-ming-lite-omni-v15>Get Started with Ming-lite-omni v1.5<a hidden class=anchor aria-hidden=true href=#get-started-with-ming-lite-omni-v15>#</a></h2><p>The model and code for Ming-lite-omni v1.5 are now open-source, and we invite everyone to try it out, share feedback, and join the discussion. Looking ahead, we&rsquo;re excited to announce that a quantized and accelerated version is on the way. This future release will not only further enhance omni-modal performance but also make the model even more lightweight, all while strengthening its multimodal reasoning and generation capabilities. Stay tuned for more updates!</p><ul><li>Github: <a href=https://github.com/inclusionAI/Ming>https://github.com/inclusionAI/Ming</a></li><li>Hugging Face: <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5</a></li><li>ModelScope: <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5</a></li></ul></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://Biao-Gong.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>