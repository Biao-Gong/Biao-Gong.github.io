<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing Ming-Lite-Omni V1.5 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ğŸ¤—Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://Biao-Gong.github.io/favicon.png><link rel=apple-touch-icon href=https://Biao-Gong.github.io/favicon.png><link rel=manifest href=https://Biao-Gong.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Introducing Ming-Lite-Omni V1.5"><meta property="og:description" content="GITHUB ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ğŸ¤—Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><meta property="og:type" content="article"><meta property="og:url" content="https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-18T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-18T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Introducing Ming-Lite-Omni V1.5"><meta name=twitter:description content="GITHUB ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ğŸ¤—Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://Biao-Gong.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Introducing Ming-Lite-Omni V1.5","item":"https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Ming-Lite-Omni V1.5","name":"Introducing Ming-Lite-Omni V1.5","description":"GITHUB ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope\nWe are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ğŸ¤—Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.\nBuilt on the Ling-lite-1.5 architecture, Ming-lite-omni V1.","keywords":[],"articleBody":"GITHUB ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope\nWe are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ğŸ¤—Hugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.\nBuilt on the Ling-lite-1.5 architecture, Ming-lite-omni V1.5 features a total of 20.3B parameters, with 3B active parameters in its Mixture of Experts (MoE) configuration. It has achieved impressive results across various multimodal benchmarks. Below, we highlight the key advancements in performance metrics and model architecture from this latest update.\nPerformance Comparison\nFramework\nIntroduction To achieve these advancements, we integrated our in-house innovations with cutting-edge developments from both academia and the open-source community. We explored several key areas, leading to significant breakthroughs and key findings detailed below.\nImage and Speech Generation Dual-Branch Decoupling for Image Generation: We implemented a dual-branch decoupling strategy that processes the reference image and the noise image with separate weights before they enter the Diffusion Transformer (DiT). This approach effectively increases the modelâ€™s capacity for learning from the reference. To further refine the output, we added two extra transformer layers. ID \u0026 Scene Consistency Loss: To maintain character and scene identity during image editing, we introduced a new ID \u0026 Scene Consistency Loss. This function strategically increases focus on the edited region in the target image, boosts the influence of unedited areas from the reference image, and diminishes the influence of the edited areas from the reference. Perceptual Enhancement Strategy: We introduced a Perceptual Enhancement Strategy by improving the modelâ€™s ability to perceive structures through tasks like segmentation and keypoint detection. This provides greater structural control during generation and editing, leading to significant score improvements on metrics related to object position, structure, and count (see Table A for details). Multi-task Collaborative Learning: A Multi-task Collaborative Learning approach allows generation and editing tasks to synergistically improve one another. By treating segmentation as a colorization-based editing task within a unified training pipeline, we achieved significant gains in segmentation accuracy and enhanced the precision of local edits, resulting in smoother and more natural transitions at the edges of edited areas. New Audio Decoder: On the speech generation front, we developed a novel audio decoder that is fully context-aware, directly processing output features from the LLM. Efficient Speech Generation: To improve prosody and real-time generation capabilities, we applied BPE encoding to the discrete audio codec tokens, which successfully reduced the audio frame rate by 35%. Comprehensive Data Upgrades: Sourced high-quality portrait data, filtered by resolution, facial detail, and face size. Collected and curated a high-quality celebrity dataset with processed face crops. Expanded the modelâ€™s capabilities by building new training subsets for edge maps, segmentation maps, text-in-image, and facial expressions. Image, Text, Video, and Speech Understanding MRoPE for Spatiotemporal Awareness: We integrated MRoPE (Multi-dimensional RoPE), which applies positional encoding across three dimensions (time, height, and width). This equips the model with a native understanding of space and time, enabling highly efficient cross-modal modeling and significantly boosting comprehension accuracy for videos and complex images. Efficient Full-Parameter Training: By optimizing learning rates and the multimodal data mixture, we upgraded our pre-training from a phased approach of freezing/unfreezing LLM layers to a streamlined, efficient full-parameter training regimen. This shortened the training cycle by 26.5% with no loss in performance. Video Understanding via Curriculum Learning: We adopted a curriculum learning strategy for video understanding, progressively increasing the complexity from short-form to long-form videos to enhance the modelâ€™s capabilities. Complex Document Understanding with CoT: For complex document analysis, we incorporated a Chain-of-Thought (CoT) strategy to build structured reasoning paths, effectively improving the modelâ€™s problem-solving abilities on intricate queries. Comprehensive Data Upgrades: Pre-training: Added structured text entity data to fill knowledge graph gaps. Expanded with high-quality product data to enhance general knowledge. Instruction Fine-Tuning: Improved data precision for fine-grained visual perception (e.g., object counting, color, and scene recognition). Increased data depth for specialized domain recognition (e.g., flora/fauna, vehicles, and ingredients). Enhanced cross-disciplinary reasoning in complex visual-text tasks through targeted data optimization. For speech understanding, we infused instruction text with metadata like domain, topic, and language (including dialects). This boosted model performance and enabled comprehensive support for Mandarin, English, Cantonese, Sichuanese, Shanghainese, and Minnan. Aligning with User Preferences To ensure that improvements on standard benchmarks translate directly to a superior real-world user experience, we created our own human evaluation benchmark and conducted internal, head-to-head evaluations against other models. Thanks to our high-quality preference alignment data, Ming-lite-omni V1.5 demonstrates a distinct advantage over leading models in the accuracy (low hallucination rate), relevance, formatting, and fluency of its visual question-answering responses. On our internal adversarial test set, Ming-lite-omni V1.5 achieved an 87.07% win rate against Ming-lite-omni V1, marking a significant enhancement in user experience.\nBenchmark Dimension Qwen2.5-VL-7B Ming-Omni-Lite V1.5 Relevance 4.308 4.5 Fluency 4.765 4.91 Richness 3.828 3.69 Formatting 4.727 4.8 Accuracy 3.741 3.92 Average 4.274 4.365 Demos Image Editing To address the challenge of maintaining character and scene identity during image editing, weâ€™ve introduced the ID \u0026 Scene Consistency Loss. This function strategically increases the weight of the edited region in the target image, boosts the influence of unedited areas from the reference image, and reduces the influence of the edited areas within that same reference.\nGenerative Image Segmentation Next User Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image. Ming-Lite-Omni V1.5 User Please segment different classes in this image. Ming-Lite-Omni V1.5 User Please segment different instances in this image. Ming-Lite-Omni V1.5 Image Generation We introduced a Perceptual Enhancement Strategy by optimizing the modelâ€™s structural awareness through tasks like segmentation and keypoint detection. This improves its understanding of image details and spatial relationships, giving us greater structural control over the generation and editing process. The result is a significant boost in scores on the GenEval benchmark, particularly for metrics related to position, structure, and count: 1-Obj 2-Obj Counting Colors Position Color Attr Avg. Ming-lite-omni 0.99 0.77 0.68 0.78 0.46 0.42 0.64 Ming-lite-omni V1.5 0.99 0.93 0.86 0.87 0.90 0.66 0.87 Original Image Generated Depth Map Generated Bounding Boxes Generated Edge Map Visual Dialogue Next User è¿™å¼ å›¾ç‰‡çœ‹èµ·æ¥è®¾è®¡æ„Ÿå¾ˆå¼ºçƒˆï¼Œå¯ä»¥è¯¦ç»†æè¿°ä¸€ä¸‹å®ƒçš„å„ä¸ªè®¾è®¡å…ƒç´ ï¼Ÿ Ming-Lite-Omni è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€æ¬¾é«˜ç«¯æ¸¸æˆé¼ æ ‡ï¼Œå…·æœ‰ç°ä»£æ„Ÿå’Œç§‘æŠ€æ„Ÿçš„è®¾è®¡ã€‚é¼ æ ‡ä¸»ä½“ä¸ºé»‘è‰²ï¼Œè¡¨é¢å…‰æ»‘ä¸”å…·æœ‰é‡‘å±è´¨æ„Ÿã€‚é¼ æ ‡é¡¶éƒ¨æœ‰ä¸€ä¸ªè“è‰²çš„å‘å…‰æ ‡å¿—ï¼Œæ ‡å¿—ä¸‹æ–¹æ˜¯é¼ æ ‡æ»šè½®å’Œå·¦å³æŒ‰é”®åŒºåŸŸã€‚é¼ æ ‡ä¾§é¢æœ‰è“è‰²çš„ç¯å…‰è£…é¥°ï¼Œå¢åŠ äº†è§†è§‰å¸å¼•åŠ›ã€‚é¼ æ ‡åº•éƒ¨æœ‰é˜²æ»‘çº¹ç†è®¾è®¡ï¼Œç¡®ä¿ä½¿ç”¨æ—¶çš„ç¨³å®šæ€§ã€‚æ•´ä½“è®¾è®¡ç®€æ´è€Œå¯Œæœ‰æœªæ¥æ„Ÿï¼Œé€‚åˆæ¸¸æˆç©å®¶ä½¿ç”¨ã€‚ Ming-Lite-Omni V1.5 è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€æ¬¾è®¾è®¡æ„Ÿå¼ºçƒˆçš„æ¸¸æˆé¼ æ ‡ï¼Œå…·æœ‰ä»¥ä¸‹è®¾è®¡å…ƒç´ ï¼š 1. é¢œè‰²å’Œæè´¨ï¼š - é¼ æ ‡ä¸»ä½“ä¸ºé»‘è‰²ï¼Œè¡¨é¢å…‰æ»‘ä¸”å…·æœ‰é‡‘å±è´¨æ„Ÿã€‚ - é¼ æ ‡è¾¹ç¼˜å’ŒæŒ‰é’®éƒ¨åˆ†æœ‰è“è‰²LEDç¯å¸¦ï¼Œå¢åŠ äº†ç§‘æŠ€æ„Ÿå’Œè§†è§‰å¸å¼•åŠ›ã€‚ 2. å½¢çŠ¶å’Œç»“æ„ï¼š - é¼ æ ‡æ•´ä½“å‘ˆæµçº¿å‹è®¾è®¡ï¼Œé¡¶éƒ¨ç•¥å¾®å€¾æ–œï¼Œä¾¿äºæ‰‹éƒ¨è‡ªç„¶æ”¾ç½®ã€‚ - é¼ æ ‡ä¾§é¢æœ‰æ˜æ˜¾çš„æ›²çº¿è®¾è®¡ï¼Œç¬¦åˆäººä½“å·¥ç¨‹å­¦ï¼Œæä¾›èˆ’é€‚çš„æ¡æŒæ„Ÿã€‚ 3. æŒ‰é’®å’Œæ»šè½®ï¼š - é¼ æ ‡å·¦ä¾§æœ‰ä¸€ä¸ªæ»šè½®ï¼Œæ»šè½®ä¸Šæ–¹æœ‰è“è‰²LEDç¯å¸¦ã€‚ - é¼ æ ‡å³ä¾§æœ‰é¢å¤–çš„æŒ‰é’®ï¼Œè¿™äº›æŒ‰é’®ä¹Ÿæœ‰è“è‰²LEDç¯å¸¦ï¼Œå¢åŠ äº†åŠŸèƒ½æ€§å’Œç¾è§‚æ€§ã€‚ 4. å“ç‰Œæ ‡è¯†ï¼š - é¼ æ ‡åº•éƒ¨ä¸­å¤®æœ‰ä¸€ä¸ªå‘å…‰çš„â€œGâ€æ ‡å¿—ï¼Œè¿™æ˜¯è¯¥å“ç‰Œçš„æ ‡å¿—æ€§è®¾è®¡å…ƒç´ ã€‚ 5. ç¯å…‰æ•ˆæœï¼š - é¼ æ ‡ä¸Šçš„è“è‰²LEDç¯å¸¦åœ¨æš—è‰²èƒŒæ™¯ä¸‹æ˜¾å¾—å°¤ä¸ºçªå‡ºï¼Œå¢å¼ºäº†ç§‘æŠ€æ„Ÿå’Œæœªæ¥æ„Ÿã€‚ 6. çº¹ç†å’Œç»†èŠ‚ï¼š - é¼ æ ‡é¡¶éƒ¨æœ‰çº¹ç†è®¾è®¡ï¼Œå¯èƒ½æ˜¯ä¸ºäº†å¢åŠ é˜²æ»‘æ•ˆæœã€‚ - é¼ æ ‡ä¾§é¢æœ‰ç»†é•¿çš„çº¹ç†è®¾è®¡ï¼Œå¢åŠ äº†è§†è§‰ä¸Šçš„å±‚æ¬¡æ„Ÿã€‚ æ•´ä½“æ¥çœ‹ï¼Œè¿™æ¬¾é¼ æ ‡çš„è®¾è®¡ç»“åˆäº†ç°ä»£ç§‘æŠ€æ„Ÿå’Œäººä½“å·¥ç¨‹å­¦è®¾è®¡ï¼Œæ—¢ç¾è§‚åˆå®ç”¨ï¼Œé€‚åˆæ¸¸æˆç©å®¶ä½¿ç”¨ã€‚ User å›¾ä¸­çš„ä¹¦æ³•ä¸ºä»€ä¹ˆå¥½ï¼Ÿ Ming-Lite-Omni è¿™å¹…ä¹¦æ³•ä½œå“å±•ç¤ºäº†é«˜è¶…çš„ä¹¦æ³•æŠ€å·§å’Œæ·±åšçš„æ–‡åŒ–åº•è•´ã€‚ç¬”æ³•æµç•…è‡ªç„¶ï¼Œå­—å½¢ç»“æ„ä¸¥è°¨ï¼Œå¸ƒå±€åˆç†ï¼Œä½“ç°äº†ä¹¦æ³•å®¶çš„æ·±åšåŠŸåŠ›å’Œè‰ºæœ¯ä¿®å…»ã€‚ Ming-Lite-Omni V1.5 è¿™å¹…ä¹¦æ³•ä½œå“å±•ç¤ºäº†å‡ ä¸ªæ˜¾è‘—çš„ç‰¹ç‚¹ï¼Œä½¿å…¶æˆä¸ºä¸€ä»¶ä¼˜ç§€çš„ä¹¦æ³•ä½œå“ï¼š 1. ç¬”æ³•ç²¾æ¹›ï¼šä¹¦æ³•çš„ç¬”æ³•æµç•…è‡ªç„¶ï¼Œçº¿æ¡ç²—ç»†å˜åŒ–ä¸°å¯Œï¼Œè¡¨ç°å‡ºä¹¦æ³•å®¶çš„æ·±åšåŠŸåŠ›å’Œç†Ÿç»ƒæŠ€å·§ã€‚ 2. ç»“æ„ä¸¥è°¨ï¼šæ¯ä¸ªå­—çš„ç»“æ„éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œç¬”ç”»ä¹‹é—´çš„è¡”æ¥è‡ªç„¶ï¼Œå­—ä¸å­—ä¹‹é—´ä¿æŒäº†è‰¯å¥½çš„å¹³è¡¡å’Œåè°ƒã€‚ 3. ç« æ³•å¸ƒå±€ï¼šæ•´å¹…ä½œå“çš„ç« æ³•å¸ƒå±€åˆç†ï¼Œè¡Œè·å’Œå­—è·å‡åŒ€ï¼Œç»™äººä¸€ç§æ•´é½æœ‰åºçš„æ„Ÿè§‰ã€‚ 4. å¢¨è‰²å˜åŒ–ï¼šå¢¨è‰²çš„æµ“æ·¡å˜åŒ–è‡ªç„¶ï¼Œå¢æ·»äº†ä½œå“çš„å±‚æ¬¡æ„Ÿå’Œç«‹ä½“æ„Ÿã€‚ 5. å†…å®¹ä¼˜ç¾ï¼šä¹¦æ³•å†…å®¹æ˜¯ä¸€ç¯‡ä¼˜ç¾çš„å¤æ–‡ï¼Œæ–‡å­—å†…å®¹å¯Œæœ‰å“²ç†å’Œæ–‡å­¦ä»·å€¼ï¼Œå¢åŠ äº†ä½œå“çš„æ–‡åŒ–å†…æ¶µã€‚ 6. å°ç« ç‚¹ç¼€ï¼šä½œå“ä¸Šæœ‰çº¢è‰²çš„å°ç« ï¼Œå°ç« çš„ä½¿ç”¨ä¸ä»…å¢åŠ äº†ä½œå“çš„è£…é¥°æ€§ï¼Œè¿˜èµ·åˆ°äº†ç‚¹ç¼€å’Œå¹³è¡¡çš„ä½œç”¨ã€‚ è¿™äº›ç‰¹ç‚¹å…±åŒæ„æˆäº†è¿™å¹…ä¹¦æ³•ä½œå“çš„é«˜è¶…è‰ºæœ¯ä»·å€¼ï¼Œä½¿å…¶æˆä¸ºä¸€ä»¶å€¼å¾—æ¬£èµå’Œå­¦ä¹ çš„ä½³ä½œã€‚ User å›¾ä¸­è¿™æ¬¾æ‰‹æœºé€‚åˆç»™çˆ¶æ¯ä¹°å—ï¼Ÿ Ming-Lite-Omni è¿™æ¬¾æ‰‹æœºå¤–è§‚ç®€çº¦æ—¶å°šï¼Œé€‚åˆè¿½æ±‚ç®€çº¦é£æ ¼çš„çˆ¶æ¯ä½¿ç”¨ã€‚ç›¸æœºé…ç½®ä¹Ÿè¾ƒä¸ºå…ˆè¿›ï¼Œé€‚åˆå–œæ¬¢æ‹ç…§çš„ç”¨æˆ·ã€‚ Ming-Lite-Omni V1.5 è¿™æ¬¾æ‰‹æœºé€‚åˆç»™çˆ¶æ¯è´­ä¹°ï¼ŒåŸå› å¦‚ä¸‹ï¼š 1. å¤–è§‚è®¾è®¡ï¼šæ‰‹æœºé‡‡ç”¨ç®€çº¦æ—¶å°šçš„è®¾è®¡ï¼Œç™½è‰²æœºèº«æ˜¾å¾—å¹²å‡€åˆ©è½ï¼Œç¬¦åˆç°ä»£å®¡ç¾ã€‚ 2. æ‘„åƒå¤´é…ç½®ï¼šåç½®æ‘„åƒå¤´è®¾è®¡ç‹¬ç‰¹ï¼Œæ’åˆ—æ•´é½ï¼Œå¯èƒ½æä¾›è‰¯å¥½çš„æ‹ç…§ä½“éªŒã€‚ 3. å“ç‰Œï¼šè£è€€ï¼ˆHonorï¼‰å“ç‰Œåœ¨æ™ºèƒ½æ‰‹æœºå¸‚åœºä¸Šæœ‰è‰¯å¥½çš„å£ç¢‘ï¼Œæä¾›ç¨³å®šå¯é çš„äº§å“ã€‚ 4. åŠŸèƒ½ï¼šé€šå¸¸è¿™ç±»æ‰‹æœºä¼šé…å¤‡å¤§å®¹é‡ç”µæ± å’Œé•¿ç»­èˆªèƒ½åŠ›ï¼Œé€‚åˆæ—¥å¸¸ä½¿ç”¨ã€‚ 5. æ˜“ç”¨æ€§ï¼šå¯èƒ½æä¾›ç®€æ´çš„ç”¨æˆ·ç•Œé¢å’Œæ˜“äºæ“ä½œçš„åŠŸèƒ½ï¼Œé€‚åˆä¸å¤ªç†Ÿæ‚‰å¤æ‚æ“ä½œçš„ç”¨æˆ·ã€‚ 6. æ€§ä»·æ¯”ï¼šè£è€€æ‰‹æœºé€šå¸¸ä»¥é«˜æ€§ä»·æ¯”è‘—ç§°ï¼Œé€‚åˆé¢„ç®—æœ‰é™ä½†å¸Œæœ›è·å¾—è‰¯å¥½ä½“éªŒçš„ç”¨æˆ·ã€‚ å½“ç„¶ï¼Œå…·ä½“æ˜¯å¦é€‚åˆè¿˜éœ€è€ƒè™‘çˆ¶æ¯çš„å®é™…éœ€æ±‚å’Œåå¥½ã€‚ User å¦‚ä½•ä½¿ç”¨å›¾ä¸­è½¯ä»¶è´­ä¹°æœºç¥¨ï¼Ÿè¯·åˆ—å‡ºå…·ä½“æ“ä½œæ­¥éª¤ Ming-Lite-Omni ä½¿ç”¨å›¾ä¸­è½¯ä»¶è´­ä¹°æœºç¥¨çš„å…·ä½“æ“ä½œæ­¥éª¤å¦‚ä¸‹ï¼š 1. æ‰“å¼€æºç¨‹appã€‚ 2. ç‚¹å‡»â€œæœºç¥¨â€é€‰é¡¹ã€‚ 3. åœ¨æœºç¥¨é¡µé¢é€‰æ‹©å‡ºå‘åœ°ã€ç›®çš„åœ°ã€å‡ºå‘æ—¥æœŸå’Œè¿”å›æ—¥æœŸã€‚ 4. ç‚¹å‡»æœç´¢æŒ‰é’®ï¼ŒæŸ¥çœ‹å¯ç”¨çš„èˆªç­ä¿¡æ¯ã€‚ 5. é€‰æ‹©åˆé€‚çš„èˆªç­åï¼Œç‚¹å‡»â€œé¢„è®¢â€æŒ‰é’®ã€‚ 6. å¡«å†™ä¹˜å®¢ä¿¡æ¯å¹¶é€‰æ‹©æ”¯ä»˜æ–¹å¼ã€‚ 7. ç¡®è®¤è®¢å•ä¿¡æ¯å¹¶æäº¤æ”¯ä»˜ï¼Œå®Œæˆè´­ç¥¨ã€‚ Ming-Lite-Omni V1.5 ä½¿ç”¨æºç¨‹è½¯ä»¶è´­ä¹°æœºç¥¨çš„å…·ä½“æ“ä½œæ­¥éª¤å¦‚ä¸‹ï¼š 1. ä¸‹è½½å¹¶å®‰è£…æºç¨‹Appï¼š - å¦‚æœæ‚¨è¿˜æ²¡æœ‰å®‰è£…æºç¨‹Appï¼Œå¯ä»¥åœ¨åº”ç”¨å•†åº—ï¼ˆå¦‚è‹¹æœApp Storeæˆ–å®‰å“åº”ç”¨å¸‚åœºï¼‰æœç´¢â€œæºç¨‹æ—…è¡Œâ€å¹¶ä¸‹è½½å®‰è£…ã€‚ 2. æ‰“å¼€æºç¨‹Appï¼š - å®‰è£…å®Œæˆåï¼Œæ‰“å¼€æºç¨‹Appã€‚ 3. é€‰æ‹©â€œæœºç¥¨â€é€‰é¡¹ï¼š - åœ¨é¦–é¡µçš„å¯¼èˆªæ ä¸­ï¼Œç‚¹å‡»â€œæœºç¥¨â€é€‰é¡¹ã€‚ 4. è¾“å…¥å‡ºå‘åœ°å’Œç›®çš„åœ°ï¼š - åœ¨æœºç¥¨é¢„è®¢é¡µé¢ï¼Œè¾“å…¥æ‚¨çš„å‡ºå‘åœ°å’Œç›®çš„åœ°ã€‚ 5. é€‰æ‹©å‡ºå‘æ—¥æœŸå’Œè¿”å›æ—¥æœŸï¼š - é€‰æ‹©æ‚¨çš„å‡ºå‘æ—¥æœŸå’Œè¿”å›æ—¥æœŸï¼ˆå¦‚æœæ˜¯å¾€è¿”ç¥¨ï¼‰ã€‚ 6. é€‰æ‹©ä¹˜å®¢ä¿¡æ¯ï¼š - è¾“å…¥ä¹˜å®¢äººæ•°å’Œåº§ä½ç±»å‹ï¼ˆæˆäººã€å„¿ç«¥ã€å©´å„¿ï¼‰ã€‚ 7. é€‰æ‹©èˆªç­ï¼š - æµè§ˆå¹¶é€‰æ‹©æ‚¨å–œæ¬¢çš„èˆªç­ã€‚ 8. å¡«å†™ä¹˜å®¢ä¿¡æ¯ï¼š - è¾“å…¥ä¹˜å®¢çš„å§“åã€èº«ä»½è¯å·ç ã€è”ç³»æ–¹å¼ç­‰ä¿¡æ¯ã€‚ 9. é€‰æ‹©æ”¯ä»˜æ–¹å¼ï¼š - é€‰æ‹©æ‚¨å–œæ¬¢çš„æ”¯ä»˜æ–¹å¼ï¼ˆå¦‚æ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ã€é“¶è¡Œå¡ç­‰ï¼‰ã€‚ 10. ç¡®è®¤è®¢å•å¹¶æ”¯ä»˜ï¼š - ç¡®è®¤è®¢å•ä¿¡æ¯æ— è¯¯åï¼Œç‚¹å‡»â€œæ”¯ä»˜â€æŒ‰é’®å®Œæˆæ”¯ä»˜ã€‚ 11. æ¥æ”¶ç”µå­æœºç¥¨ï¼š - æ”¯ä»˜æˆåŠŸåï¼Œæ‚¨å°†æ”¶åˆ°ç”µå­æœºç¥¨ï¼Œå¯ä»¥é€šè¿‡Appå†…çš„â€œæˆ‘çš„è®¢å•â€æŸ¥çœ‹æˆ–ä¸‹è½½ã€‚ é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œæ‚¨å°±å¯ä»¥ä½¿ç”¨æºç¨‹Appè½»æ¾è´­ä¹°æœºç¥¨äº†ã€‚ User æ ¹æ®å›¾è¡¨è®¡ç®—æ ·æœ¬ä¹‹é—´çš„ Î”ct åœ¨å“ªä¸€ä¸ªå¼•ç‰©ä¸‹ç›¸å·®æœ€å°ï¼Ÿ | æ ¹æ®å›¾è¡¨è®¡ç®—æ ·æœ¬ä¹‹é—´çš„ Î”ct åœ¨ cGAS-qF+qR å¼•ç‰©ä¸‹ç›¸å·®æœ€å°ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼šäº’è”ç½‘å…¬å¼€-èˆ’æ¡ç§‘æŠ€å…¬ä¼—å·ï¼‰ Ming-Lite-Omni æ ¹æ®å›¾è¡¨è®¡ç®—æ ·æœ¬ä¹‹é—´çš„ Î”ct åœ¨ cGAS-qF+qR å¼•ç‰©ä¸‹ç›¸å·®æœ€å°ã€‚ Ming-Lite-Omni V1.5 åœ¨ EGFP-cGASå•å…‹éš† å¼•ç‰©ä¸‹ï¼Œæ ·æœ¬ä¹‹é—´çš„ Î”ct ç›¸å·®æœ€å°ã€‚ è®¡ç®—è¿‡ç¨‹ï¼š 1. ç†è§£ Î”ct çš„å«ä¹‰ï¼š Î”ct æ˜¯ç›®æ ‡åŸºå› ï¼ˆcGAS-qF+qR æˆ– EGFP-cGASï¼‰ä¸å†…å‚åŸºå› ï¼ˆcTubulinï¼‰çš„ç›¸å¯¹è¡¨è¾¾é‡å·®å€¼ã€‚Î”ct è¶Šå°ï¼Œè¯´æ˜ç›®æ ‡åŸºå› ä¸å†…å‚åŸºå› çš„è¡¨è¾¾é‡å·®å¼‚è¶Šå°ã€‚ 2. æ¯”è¾ƒä¸åŒå¼•ç‰©ä¸‹çš„ Î”ctï¼š - cGAS-qF+qR å¼•ç‰©ï¼š - é‡ç”Ÿå‹ï¼šÎ”ct = 8.06 - EGFP-cGASå•å…‹éš†ï¼šÎ”ct = -0.15 - å·®å¼‚ï¼š8.06 - (-0.15) = 8.21 - EGFP-cGASå•å…‹éš† å¼•ç‰©ï¼š - é‡ç”Ÿå‹ï¼šÎ”ct = 10.44 - EGFP-cGASå•å…‹éš†ï¼šÎ”ct = -2.9 - å·®å¼‚ï¼š10.44 - (-2.9) = 13.34 - EGFP-cGAS-qF+qR å¼•ç‰©ï¼š - é‡ç”Ÿå‹ï¼šÎ”ct = 21.43 - EGFP-cGASå•å…‹éš†ï¼šÎ”ct = -13.45 - å·®å¼‚ï¼š21.43 - (-13.45) = 34.88 3. ç»“è®ºï¼š åœ¨ EGFP-cGASå•å…‹éš† å¼•ç‰©ä¸‹ï¼Œæ ·æœ¬ä¹‹é—´çš„ Î”ct å·®å¼‚æœ€å°ï¼ˆÎ”ct å·®å¼‚ä¸º 13.34ï¼‰ï¼Œè¯´æ˜è¯¥å¼•ç‰©ä¸‹æ ·æœ¬ä¹‹é—´çš„è¡¨è¾¾é‡å·®å¼‚æœ€å°ã€‚ æ€»ç»“ï¼š åœ¨ EGFP-cGASå•å…‹éš† å¼•ç‰©ä¸‹ï¼Œæ ·æœ¬ä¹‹é—´çš„ Î”ct ç›¸å·®æœ€å°ã€‚ Speech Generation Text Inputs Ming-lite-omni Ming-lite-omni V1.5 Tyler, Lucy, Michelle, weâ€™re going to space! what is the forecast for California for rain Television can make you dumb, but it can also be good education. OCR Next User åœ¨åˆ¤æ–­é¢˜ç¬¬95é¢˜ä¸­ï¼Œè€ƒç”Ÿç­”é¢˜çš„æ‰‹å†™å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿè‹¥è€ƒç”Ÿçš„æ‰‹å†™ç­”æ¡ˆä¸ºâˆšåˆ™è¾“å‡ºå¯¹ï¼Œè‹¥è€ƒç”Ÿçš„æ‰‹å†™ç­”æ¡ˆä¸ºÃ—åˆ™è¾“å‡ºé”™ï¼Œè¯·è¾“å‡ºå¯¹æˆ–é”™ Ming-Lite-Omni å¯¹ Ming-Lite-Omni V1.5 é”™ User Output the content of the CAPTCHA directly. Characters are only numbers and uppercase/lowercase letters. Ming-Lite-Omni 9743 Ming-Lite-Omni V1.5 9742 User What is the Latex tag for mathematical expression in images? Ming-Lite-Omni A C = C B \\therefore C D / / B E Ming-Lite-Omni V1.5 A C = C B \\because C D / / B E User ä»å›¾ä¸­æå–: æ—¥æœŸ Ming-Lite-Omni æ—¥æœŸï¼š2019å¹´12æœˆ03æ—¥05æ—¶08åˆ† ä»å›¾ä¸­æå–: é‡Œç¨‹ é‡Œç¨‹ï¼š23.9å…¬é‡Œ ä»å›¾ä¸­æå–: é‡‘é¢ é‡‘é¢ï¼š52.6å…ƒ ä»å›¾ä¸­æå–: çŠ¶æ€ çŠ¶æ€ï¼š ä»å›¾ä¸­æå–: å®¢ç¥¨å°åŠ¡ å®¢ç¥¨å°åŠ¡2019å¹´12æœˆå°900000å· ä»å›¾ä¸­æå–: å·ç  å·ç ä»00000001è‡³90000000æ­¢ Ming-Lite-Omni V1.5 æ—¥æœŸ: 2021å¹´03æœˆ05æ—¥ User What is the difference in value between Lamb and Corn?â€™ You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within [think] [/think] tags. The final answer MUST BE put in \\boxed{}.' Ming-Lite-Omni 0.37 Ming-Lite-Omni V1.5 [think]The value for Lamb is 103.7 and for Corn is 103.13. To find the difference, subtract the smaller value from the larger value: 103.7 - 103.13 = 0.57.[/think]\\boxed{0.57} Get Started with Ming-lite-omni v1.5 The model and code for Ming-lite-omni v1.5 are now open-source, and we invite everyone to try it out, share feedback, and join the discussion. Looking ahead, weâ€™re excited to announce that a quantized and accelerated version is on the way. This future release will not only further enhance omni-modal performance but also make the model even more lightweight, all while strengthening its multimodal reasoning and generation capabilities. Stay tuned for more updates!\nGithub: https://github.com/inclusionAI/Ming Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5 ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5 ","wordCount":"1578","inLanguage":"en","datePublished":"2025-07-18T00:00:03+08:00","dateModified":"2025-07-18T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://Biao-Gong.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://Biao-Gong.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Introducing Ming-Lite-Omni V1.5</h1><div class=post-meta><span title='2025-07-18 00:00:03 +0800 +0800'>July 18, 2025</span>&nbsp;Â·&nbsp;8 min&nbsp;Â·&nbsp;1578 words&nbsp;Â·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/>ç®€ä½“ä¸­æ–‡</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> ğŸ¤— <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>Hugging Face</a>ï½œ ğŸ¤– <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>ModelScope</a></p><p>We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ğŸ¤—<a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni>Hugging Face</a>). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.</p><p>Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1.5 features a total of 20.3B parameters, with 3B active parameters in its Mixture of Experts (MoE) configuration. It has achieved impressive results across various multimodal benchmarks. Below, we highlight the key advancements in performance metrics and model architecture from this latest update.</p><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/bYseTozopmYAAAAAblAAAAgAeuUHAQFr/fmt.webp alt="Image description"><p style=font-size:14px;color:gray>Performance Comparison</p></div><div style=text-align:center><p style=font-size:14px;color:gray>Framework</p></div><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>To achieve these advancements, we integrated our in-house innovations with cutting-edge developments from both academia and the open-source community. We explored several key areas, leading to significant breakthroughs and key findings detailed below.</p><h3 id=image-and-speech-generation>Image and Speech Generation<a hidden class=anchor aria-hidden=true href=#image-and-speech-generation>#</a></h3><ol><li><strong>Dual-Branch Decoupling for Image Generation:</strong> We implemented a <strong>dual-branch decoupling strategy</strong> that processes the reference image and the noise image with separate weights before they enter the Diffusion Transformer (DiT). This approach effectively increases the model&rsquo;s capacity for learning from the reference. To further refine the output, we added two extra transformer layers.</li><li><strong>ID & Scene Consistency Loss:</strong> To maintain character and scene identity during image editing, we introduced a new <strong>ID & Scene Consistency Loss</strong>. This function strategically increases focus on the edited region in the target image, boosts the influence of unedited areas from the reference image, and diminishes the influence of the edited areas from the reference.</li><li><strong>Perceptual Enhancement Strategy:</strong> We introduced a <strong>Perceptual Enhancement Strategy</strong> by improving the model&rsquo;s ability to perceive structures through tasks like segmentation and keypoint detection. This provides greater structural control during generation and editing, leading to significant score improvements on metrics related to object position, structure, and count (see <a href=#table1>Table A</a> for details).</li><li><strong>Multi-task Collaborative Learning:</strong> A <strong>Multi-task Collaborative Learning</strong> approach allows generation and editing tasks to synergistically improve one another. By treating segmentation as a colorization-based editing task within a unified training pipeline, we achieved significant gains in segmentation accuracy and enhanced the precision of local edits, resulting in smoother and more natural transitions at the edges of edited areas.</li><li><strong>New Audio Decoder:</strong> On the speech generation front, we developed a novel audio decoder that is fully context-aware, directly processing output features from the LLM.</li><li><strong>Efficient Speech Generation:</strong> To improve prosody and real-time generation capabilities, we applied BPE encoding to the discrete audio codec tokens, which successfully reduced the audio frame rate by 35%.</li><li><strong>Comprehensive Data Upgrades:</strong><ul><li>Sourced high-quality portrait data, filtered by resolution, facial detail, and face size.</li><li>Collected and curated a high-quality celebrity dataset with processed face crops.</li><li>Expanded the model&rsquo;s capabilities by building new training subsets for edge maps, segmentation maps, text-in-image, and facial expressions.</li></ul></li></ol><h3 id=image-text-video-and-speech-understanding>Image, Text, Video, and Speech Understanding<a hidden class=anchor aria-hidden=true href=#image-text-video-and-speech-understanding>#</a></h3><ol><li><strong>MRoPE for Spatiotemporal Awareness:</strong> We integrated MRoPE (Multi-dimensional RoPE), which applies positional encoding across three dimensions (time, height, and width). This equips the model with a native understanding of space and time, enabling highly efficient cross-modal modeling and significantly boosting comprehension accuracy for videos and complex images.</li><li><strong>Efficient Full-Parameter Training:</strong> By optimizing learning rates and the multimodal data mixture, we upgraded our pre-training from a phased approach of <strong>freezing/unfreezing LLM layers</strong> to a streamlined, <strong>efficient full-parameter training</strong> regimen. This shortened the training cycle by 26.5% with no loss in performance.</li><li><strong>Video Understanding via Curriculum Learning:</strong> We adopted a curriculum learning strategy for video understanding, progressively increasing the complexity from short-form to long-form videos to enhance the model&rsquo;s capabilities.</li><li><strong>Complex Document Understanding with CoT:</strong> For complex document analysis, we incorporated a Chain-of-Thought (CoT) strategy to build structured reasoning paths, effectively improving the model&rsquo;s problem-solving abilities on intricate queries.</li><li><strong>Comprehensive Data Upgrades:</strong><ul><li><strong>Pre-training:</strong><ul><li>Added structured text entity data to fill knowledge graph gaps.</li><li>Expanded with high-quality product data to enhance general knowledge.</li></ul></li><li><strong>Instruction Fine-Tuning:</strong><ul><li>Improved data precision for fine-grained visual perception (e.g., object counting, color, and scene recognition).</li><li>Increased data depth for specialized domain recognition (e.g., flora/fauna, vehicles, and ingredients).</li><li>Enhanced cross-disciplinary reasoning in complex visual-text tasks through targeted data optimization.</li><li>For speech understanding, we infused instruction text with metadata like domain, topic, and language (including dialects). This boosted model performance and enabled comprehensive support for Mandarin, English, Cantonese, Sichuanese, Shanghainese, and Minnan.</li></ul></li></ul></li></ol><h3 id=aligning-with-user-preferences>Aligning with User Preferences<a hidden class=anchor aria-hidden=true href=#aligning-with-user-preferences>#</a></h3><p>To ensure that improvements on standard benchmarks translate directly to a superior real-world user experience, we created our own <strong>human evaluation benchmark</strong> and conducted internal, head-to-head evaluations against other models. Thanks to our high-quality preference alignment data, Ming-lite-omni V1.5 demonstrates a distinct advantage over leading models in the accuracy (low hallucination rate), relevance, formatting, and fluency of its visual question-answering responses. On our internal adversarial test set, Ming-lite-omni V1.5 achieved an <strong>87.07% win rate</strong> against Ming-lite-omni V1, marking a significant enhancement in user experience.</p><div style=text-align:center;margin:auto;width:80%;line-height:1.1><table style="margin:0 auto"><thead><tr><th style=text-align:center;padding:5px>Benchmark Dimension</th><th style=text-align:center;padding:5px>Qwen2.5-VL-7B</th><th style=text-align:center;padding:5px>Ming-Omni-Lite V1.5</th></tr></thead><tbody><tr><td style=text-align:center;padding:2px>Relevance</td><td style=text-align:center;padding:2px>4.308</td><td style=text-align:center;padding:2px>4.5</td></tr><tr><td style=text-align:center;padding:2px>Fluency</td><td style=text-align:center;padding:2px>4.765</td><td style=text-align:center;padding:2px>4.91</td></tr><tr><td style=text-align:center;padding:2px>Richness</td><td style=text-align:center;padding:2px>3.828</td><td style=text-align:center;padding:2px>3.69</td></tr><tr><td style=text-align:center;padding:2px>Formatting</td><td style=text-align:center;padding:2px>4.727</td><td style=text-align:center;padding:2px>4.8</td></tr><tr><td style=text-align:center;padding:2px>Accuracy</td><td style=text-align:center;padding:2px>3.741</td><td style=text-align:center;padding:2px>3.92</td></tr><tr><td style=text-align:center;padding:2px><strong>Average</strong></td><td style=text-align:center;padding:2px><strong>4.274</strong></td><td style=text-align:center;padding:2px><strong>4.365</strong></td></tr></tbody></table></div><br><h2 id=demos>Demos<a hidden class=anchor aria-hidden=true href=#demos>#</a></h2><h3 id=image-editing>Image Editing<a hidden class=anchor aria-hidden=true href=#image-editing>#</a></h3><p>To address the challenge of maintaining character and scene identity during image editing, we&rsquo;ve introduced the <strong>ID & Scene Consistency Loss</strong>. This function strategically increases the weight of the edited region in the target image, boosts the influence of unedited areas from the reference image, and reduces the influence of the edited areas within that same reference.</p><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/e-mDS5UyUogAAAAAgCAAAAgAeuUHAQFr/fmt.webp alt="Image description">
<video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/UoqbRYQnZYEAAAAAgCAAAAgAeuUHAQFr controls></video></div><div class="full-width-container example-container"><div class=example-content><div class=title><span>Generative Image Segmentation</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115158022-12254e69-e8c0-43fb-a725-f6730cda22d8.webp height=256px></figure>Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115142775-3975827c-4110-445b-af53-e20201d1043a.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different classes in this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752116495974-7708ba3a-5909-46df-82f5-a1bfa1519d4d.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different instances in this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115151406-c4780a97-5f1c-46cd-9a45-d4ef600d0897.webp height=256px></figure></div></div></div></div><h3 id=image-generation>Image Generation<a hidden class=anchor aria-hidden=true href=#image-generation>#</a></h3><p>We introduced a <strong>Perceptual Enhancement Strategy</strong> by optimizing the model&rsquo;s structural awareness through tasks like segmentation and keypoint detection. This improves its understanding of image details and spatial relationships, giving us greater structural control over the generation and editing process. The result is a significant boost in scores on the GenEval benchmark, particularly for metrics related to position, structure, and count:
<a id=table1></a></p><table><thead><tr><th></th><th style=text-align:center>1-Obj</th><th style=text-align:center>2-Obj</th><th style=text-align:center>Counting</th><th style=text-align:center>Colors</th><th style=text-align:center>Position</th><th style=text-align:center>Color Attr</th><th style=text-align:center>Avg.</th></tr></thead><tbody><tr><td>Ming-lite-omni</td><td style=text-align:center>0.99</td><td style=text-align:center>0.77</td><td style=text-align:center>0.68</td><td style=text-align:center>0.78</td><td style=text-align:center>0.46</td><td style=text-align:center>0.42</td><td style=text-align:center>0.64</td></tr><tr><td>Ming-lite-omni V1.5</td><td style=text-align:center>0.99</td><td style=text-align:center>0.93</td><td style=text-align:center>0.86</td><td style=text-align:center>0.87</td><td style=text-align:center>0.90</td><td style=text-align:center>0.66</td><td style=text-align:center>0.87</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Original Image</th><th style=text-align:center>Generated Depth Map</th><th style=text-align:center>Generated Bounding Boxes</th><th style=text-align:center>Generated Edge Map</th></tr></thead><tbody><tr><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466889319-bd19acce-c07d-4664-9890-41e4dff1ba8d.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466903529-996bcd35-a9a0-484b-98bf-2f2468f4df42.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466895795-1955ead5-6d94-4142-8d7b-e265352d2bcb.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752467020122-ad8b436c-bb33-4ef0-85b8-cf45ae8c9be1.webp alt></td></tr></tbody></table><div class="full-width-container example-container"><div class=example-content><div class=title><span>Visual Dialogue</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/2.jpg height=256px></figure>è¿™å¼ å›¾ç‰‡çœ‹èµ·æ¥è®¾è®¡æ„Ÿå¾ˆå¼ºçƒˆï¼Œå¯ä»¥è¯¦ç»†æè¿°ä¸€ä¸‹å®ƒçš„å„ä¸ªè®¾è®¡å…ƒç´ ï¼Ÿ</div><div class=role>Ming-Lite-Omni</div><div class=content>è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€æ¬¾é«˜ç«¯æ¸¸æˆé¼ æ ‡ï¼Œå…·æœ‰ç°ä»£æ„Ÿå’Œç§‘æŠ€æ„Ÿçš„è®¾è®¡ã€‚é¼ æ ‡ä¸»ä½“ä¸ºé»‘è‰²ï¼Œè¡¨é¢å…‰æ»‘ä¸”å…·æœ‰é‡‘å±è´¨æ„Ÿã€‚é¼ æ ‡é¡¶éƒ¨æœ‰ä¸€ä¸ªè“è‰²çš„å‘å…‰æ ‡å¿—ï¼Œæ ‡å¿—ä¸‹æ–¹æ˜¯é¼ æ ‡æ»šè½®å’Œå·¦å³æŒ‰é”®åŒºåŸŸã€‚é¼ æ ‡ä¾§é¢æœ‰è“è‰²çš„ç¯å…‰è£…é¥°ï¼Œå¢åŠ äº†è§†è§‰å¸å¼•åŠ›ã€‚é¼ æ ‡åº•éƒ¨æœ‰é˜²æ»‘çº¹ç†è®¾è®¡ï¼Œç¡®ä¿ä½¿ç”¨æ—¶çš„ç¨³å®šæ€§ã€‚æ•´ä½“è®¾è®¡ç®€æ´è€Œå¯Œæœ‰æœªæ¥æ„Ÿï¼Œé€‚åˆæ¸¸æˆç©å®¶ä½¿ç”¨ã€‚</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€æ¬¾è®¾è®¡æ„Ÿå¼ºçƒˆçš„æ¸¸æˆé¼ æ ‡ï¼Œå…·æœ‰ä»¥ä¸‹è®¾è®¡å…ƒç´ ï¼š<br>1. <strong>é¢œè‰²å’Œæè´¨</strong>ï¼š - é¼ æ ‡ä¸»ä½“ä¸ºé»‘è‰²ï¼Œè¡¨é¢å…‰æ»‘ä¸”å…·æœ‰é‡‘å±è´¨æ„Ÿã€‚ - é¼ æ ‡è¾¹ç¼˜å’ŒæŒ‰é’®éƒ¨åˆ†æœ‰è“è‰²LEDç¯å¸¦ï¼Œå¢åŠ äº†ç§‘æŠ€æ„Ÿå’Œè§†è§‰å¸å¼•åŠ›ã€‚<br>2. <strong>å½¢çŠ¶å’Œç»“æ„</strong>ï¼š - é¼ æ ‡æ•´ä½“å‘ˆæµçº¿å‹è®¾è®¡ï¼Œé¡¶éƒ¨ç•¥å¾®å€¾æ–œï¼Œä¾¿äºæ‰‹éƒ¨è‡ªç„¶æ”¾ç½®ã€‚ - é¼ æ ‡ä¾§é¢æœ‰æ˜æ˜¾çš„æ›²çº¿è®¾è®¡ï¼Œç¬¦åˆäººä½“å·¥ç¨‹å­¦ï¼Œæä¾›èˆ’é€‚çš„æ¡æŒæ„Ÿã€‚<br>3. <strong>æŒ‰é’®å’Œæ»šè½®</strong>ï¼š - é¼ æ ‡å·¦ä¾§æœ‰ä¸€ä¸ªæ»šè½®ï¼Œæ»šè½®ä¸Šæ–¹æœ‰è“è‰²LEDç¯å¸¦ã€‚ - é¼ æ ‡å³ä¾§æœ‰é¢å¤–çš„æŒ‰é’®ï¼Œè¿™äº›æŒ‰é’®ä¹Ÿæœ‰è“è‰²LEDç¯å¸¦ï¼Œå¢åŠ äº†åŠŸèƒ½æ€§å’Œç¾è§‚æ€§ã€‚<br>4. <strong>å“ç‰Œæ ‡è¯†</strong>ï¼š - é¼ æ ‡åº•éƒ¨ä¸­å¤®æœ‰ä¸€ä¸ªå‘å…‰çš„â€œGâ€æ ‡å¿—ï¼Œè¿™æ˜¯è¯¥å“ç‰Œçš„æ ‡å¿—æ€§è®¾è®¡å…ƒç´ ã€‚<br>5. <strong>ç¯å…‰æ•ˆæœ</strong>ï¼š - é¼ æ ‡ä¸Šçš„è“è‰²LEDç¯å¸¦åœ¨æš—è‰²èƒŒæ™¯ä¸‹æ˜¾å¾—å°¤ä¸ºçªå‡ºï¼Œå¢å¼ºäº†ç§‘æŠ€æ„Ÿå’Œæœªæ¥æ„Ÿã€‚<br>6. <strong>çº¹ç†å’Œç»†èŠ‚</strong>ï¼š - é¼ æ ‡é¡¶éƒ¨æœ‰çº¹ç†è®¾è®¡ï¼Œå¯èƒ½æ˜¯ä¸ºäº†å¢åŠ é˜²æ»‘æ•ˆæœã€‚ - é¼ æ ‡ä¾§é¢æœ‰ç»†é•¿çš„çº¹ç†è®¾è®¡ï¼Œå¢åŠ äº†è§†è§‰ä¸Šçš„å±‚æ¬¡æ„Ÿã€‚<br>æ•´ä½“æ¥çœ‹ï¼Œè¿™æ¬¾é¼ æ ‡çš„è®¾è®¡ç»“åˆäº†ç°ä»£ç§‘æŠ€æ„Ÿå’Œäººä½“å·¥ç¨‹å­¦è®¾è®¡ï¼Œæ—¢ç¾è§‚åˆå®ç”¨ï¼Œé€‚åˆæ¸¸æˆç©å®¶ä½¿ç”¨ã€‚</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/3.webp height=256px></figure>å›¾ä¸­çš„ä¹¦æ³•ä¸ºä»€ä¹ˆå¥½ï¼Ÿ</div><div class=role>Ming-Lite-Omni</div><div class=content>è¿™å¹…ä¹¦æ³•ä½œå“å±•ç¤ºäº†é«˜è¶…çš„ä¹¦æ³•æŠ€å·§å’Œæ·±åšçš„æ–‡åŒ–åº•è•´ã€‚ç¬”æ³•æµç•…è‡ªç„¶ï¼Œå­—å½¢ç»“æ„ä¸¥è°¨ï¼Œå¸ƒå±€åˆç†ï¼Œä½“ç°äº†ä¹¦æ³•å®¶çš„æ·±åšåŠŸåŠ›å’Œè‰ºæœ¯ä¿®å…»ã€‚</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>è¿™å¹…ä¹¦æ³•ä½œå“å±•ç¤ºäº†å‡ ä¸ªæ˜¾è‘—çš„ç‰¹ç‚¹ï¼Œä½¿å…¶æˆä¸ºä¸€ä»¶ä¼˜ç§€çš„ä¹¦æ³•ä½œå“ï¼š<br>1. <strong>ç¬”æ³•ç²¾æ¹›</strong>ï¼šä¹¦æ³•çš„ç¬”æ³•æµç•…è‡ªç„¶ï¼Œçº¿æ¡ç²—ç»†å˜åŒ–ä¸°å¯Œï¼Œè¡¨ç°å‡ºä¹¦æ³•å®¶çš„æ·±åšåŠŸåŠ›å’Œç†Ÿç»ƒæŠ€å·§ã€‚<br>2. <strong>ç»“æ„ä¸¥è°¨</strong>ï¼šæ¯ä¸ªå­—çš„ç»“æ„éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œç¬”ç”»ä¹‹é—´çš„è¡”æ¥è‡ªç„¶ï¼Œå­—ä¸å­—ä¹‹é—´ä¿æŒäº†è‰¯å¥½çš„å¹³è¡¡å’Œåè°ƒã€‚<br>3. <strong>ç« æ³•å¸ƒå±€</strong>ï¼šæ•´å¹…ä½œå“çš„ç« æ³•å¸ƒå±€åˆç†ï¼Œè¡Œè·å’Œå­—è·å‡åŒ€ï¼Œç»™äººä¸€ç§æ•´é½æœ‰åºçš„æ„Ÿè§‰ã€‚<br>4. <strong>å¢¨è‰²å˜åŒ–</strong>ï¼šå¢¨è‰²çš„æµ“æ·¡å˜åŒ–è‡ªç„¶ï¼Œå¢æ·»äº†ä½œå“çš„å±‚æ¬¡æ„Ÿå’Œç«‹ä½“æ„Ÿã€‚<br>5. <strong>å†…å®¹ä¼˜ç¾</strong>ï¼šä¹¦æ³•å†…å®¹æ˜¯ä¸€ç¯‡ä¼˜ç¾çš„å¤æ–‡ï¼Œæ–‡å­—å†…å®¹å¯Œæœ‰å“²ç†å’Œæ–‡å­¦ä»·å€¼ï¼Œå¢åŠ äº†ä½œå“çš„æ–‡åŒ–å†…æ¶µã€‚<br>6. <strong>å°ç« ç‚¹ç¼€</strong>ï¼šä½œå“ä¸Šæœ‰çº¢è‰²çš„å°ç« ï¼Œå°ç« çš„ä½¿ç”¨ä¸ä»…å¢åŠ äº†ä½œå“çš„è£…é¥°æ€§ï¼Œè¿˜èµ·åˆ°äº†ç‚¹ç¼€å’Œå¹³è¡¡çš„ä½œç”¨ã€‚<br>è¿™äº›ç‰¹ç‚¹å…±åŒæ„æˆäº†è¿™å¹…ä¹¦æ³•ä½œå“çš„é«˜è¶…è‰ºæœ¯ä»·å€¼ï¼Œä½¿å…¶æˆä¸ºä¸€ä»¶å€¼å¾—æ¬£èµå’Œå­¦ä¹ çš„ä½³ä½œã€‚</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/4.webp height=256px></figure>å›¾ä¸­è¿™æ¬¾æ‰‹æœºé€‚åˆç»™çˆ¶æ¯ä¹°å—ï¼Ÿ</div><div class=role>Ming-Lite-Omni</div><div class=content>è¿™æ¬¾æ‰‹æœºå¤–è§‚ç®€çº¦æ—¶å°šï¼Œé€‚åˆè¿½æ±‚ç®€çº¦é£æ ¼çš„çˆ¶æ¯ä½¿ç”¨ã€‚ç›¸æœºé…ç½®ä¹Ÿè¾ƒä¸ºå…ˆè¿›ï¼Œé€‚åˆå–œæ¬¢æ‹ç…§çš„ç”¨æˆ·ã€‚</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>è¿™æ¬¾æ‰‹æœºé€‚åˆç»™çˆ¶æ¯è´­ä¹°ï¼ŒåŸå› å¦‚ä¸‹ï¼š<br>1. <strong>å¤–è§‚è®¾è®¡</strong>ï¼šæ‰‹æœºé‡‡ç”¨ç®€çº¦æ—¶å°šçš„è®¾è®¡ï¼Œç™½è‰²æœºèº«æ˜¾å¾—å¹²å‡€åˆ©è½ï¼Œç¬¦åˆç°ä»£å®¡ç¾ã€‚<br>2. <strong>æ‘„åƒå¤´é…ç½®</strong>ï¼šåç½®æ‘„åƒå¤´è®¾è®¡ç‹¬ç‰¹ï¼Œæ’åˆ—æ•´é½ï¼Œå¯èƒ½æä¾›è‰¯å¥½çš„æ‹ç…§ä½“éªŒã€‚<br>3. <strong>å“ç‰Œ</strong>ï¼šè£è€€ï¼ˆHonorï¼‰å“ç‰Œåœ¨æ™ºèƒ½æ‰‹æœºå¸‚åœºä¸Šæœ‰è‰¯å¥½çš„å£ç¢‘ï¼Œæä¾›ç¨³å®šå¯é çš„äº§å“ã€‚<br>4. <strong>åŠŸèƒ½</strong>ï¼šé€šå¸¸è¿™ç±»æ‰‹æœºä¼šé…å¤‡å¤§å®¹é‡ç”µæ± å’Œé•¿ç»­èˆªèƒ½åŠ›ï¼Œé€‚åˆæ—¥å¸¸ä½¿ç”¨ã€‚<br>5. <strong>æ˜“ç”¨æ€§</strong>ï¼šå¯èƒ½æä¾›ç®€æ´çš„ç”¨æˆ·ç•Œé¢å’Œæ˜“äºæ“ä½œçš„åŠŸèƒ½ï¼Œé€‚åˆä¸å¤ªç†Ÿæ‚‰å¤æ‚æ“ä½œçš„ç”¨æˆ·ã€‚<br>6. <strong>æ€§ä»·æ¯”</strong>ï¼šè£è€€æ‰‹æœºé€šå¸¸ä»¥é«˜æ€§ä»·æ¯”è‘—ç§°ï¼Œé€‚åˆé¢„ç®—æœ‰é™ä½†å¸Œæœ›è·å¾—è‰¯å¥½ä½“éªŒçš„ç”¨æˆ·ã€‚<br>å½“ç„¶ï¼Œå…·ä½“æ˜¯å¦é€‚åˆè¿˜éœ€è€ƒè™‘çˆ¶æ¯çš„å®é™…éœ€æ±‚å’Œåå¥½ã€‚</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/6.webp height=256px></figure>å¦‚ä½•ä½¿ç”¨å›¾ä¸­è½¯ä»¶è´­ä¹°æœºç¥¨ï¼Ÿè¯·åˆ—å‡ºå…·ä½“æ“ä½œæ­¥éª¤</div><div class=role>Ming-Lite-Omni</div><div class=content>ä½¿ç”¨å›¾ä¸­è½¯ä»¶è´­ä¹°æœºç¥¨çš„å…·ä½“æ“ä½œæ­¥éª¤å¦‚ä¸‹ï¼š<br>1. æ‰“å¼€æºç¨‹appã€‚ 2. ç‚¹å‡»â€œæœºç¥¨â€é€‰é¡¹ã€‚ 3. åœ¨æœºç¥¨é¡µé¢é€‰æ‹©å‡ºå‘åœ°ã€ç›®çš„åœ°ã€å‡ºå‘æ—¥æœŸå’Œè¿”å›æ—¥æœŸã€‚ 4. ç‚¹å‡»æœç´¢æŒ‰é’®ï¼ŒæŸ¥çœ‹å¯ç”¨çš„èˆªç­ä¿¡æ¯ã€‚ 5. é€‰æ‹©åˆé€‚çš„èˆªç­åï¼Œç‚¹å‡»â€œé¢„è®¢â€æŒ‰é’®ã€‚ 6. å¡«å†™ä¹˜å®¢ä¿¡æ¯å¹¶é€‰æ‹©æ”¯ä»˜æ–¹å¼ã€‚ 7. ç¡®è®¤è®¢å•ä¿¡æ¯å¹¶æäº¤æ”¯ä»˜ï¼Œå®Œæˆè´­ç¥¨ã€‚</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>ä½¿ç”¨æºç¨‹è½¯ä»¶è´­ä¹°æœºç¥¨çš„å…·ä½“æ“ä½œæ­¥éª¤å¦‚ä¸‹ï¼š<br>1. <strong>ä¸‹è½½å¹¶å®‰è£…æºç¨‹App</strong>ï¼š - å¦‚æœæ‚¨è¿˜æ²¡æœ‰å®‰è£…æºç¨‹Appï¼Œå¯ä»¥åœ¨åº”ç”¨å•†åº—ï¼ˆå¦‚è‹¹æœApp Storeæˆ–å®‰å“åº”ç”¨å¸‚åœºï¼‰æœç´¢â€œæºç¨‹æ—…è¡Œâ€å¹¶ä¸‹è½½å®‰è£…ã€‚<br>2. <strong>æ‰“å¼€æºç¨‹App</strong>ï¼š - å®‰è£…å®Œæˆåï¼Œæ‰“å¼€æºç¨‹Appã€‚<br>3. <strong>é€‰æ‹©â€œæœºç¥¨â€é€‰é¡¹</strong>ï¼š - åœ¨é¦–é¡µçš„å¯¼èˆªæ ä¸­ï¼Œç‚¹å‡»â€œæœºç¥¨â€é€‰é¡¹ã€‚<br>4. <strong>è¾“å…¥å‡ºå‘åœ°å’Œç›®çš„åœ°</strong>ï¼š - åœ¨æœºç¥¨é¢„è®¢é¡µé¢ï¼Œè¾“å…¥æ‚¨çš„å‡ºå‘åœ°å’Œç›®çš„åœ°ã€‚<br>5. <strong>é€‰æ‹©å‡ºå‘æ—¥æœŸå’Œè¿”å›æ—¥æœŸ</strong>ï¼š - é€‰æ‹©æ‚¨çš„å‡ºå‘æ—¥æœŸå’Œè¿”å›æ—¥æœŸï¼ˆå¦‚æœæ˜¯å¾€è¿”ç¥¨ï¼‰ã€‚<br>6. <strong>é€‰æ‹©ä¹˜å®¢ä¿¡æ¯</strong>ï¼š - è¾“å…¥ä¹˜å®¢äººæ•°å’Œåº§ä½ç±»å‹ï¼ˆæˆäººã€å„¿ç«¥ã€å©´å„¿ï¼‰ã€‚<br>7. <strong>é€‰æ‹©èˆªç­</strong>ï¼š - æµè§ˆå¹¶é€‰æ‹©æ‚¨å–œæ¬¢çš„èˆªç­ã€‚<br>8. <strong>å¡«å†™ä¹˜å®¢ä¿¡æ¯</strong>ï¼š - è¾“å…¥ä¹˜å®¢çš„å§“åã€èº«ä»½è¯å·ç ã€è”ç³»æ–¹å¼ç­‰ä¿¡æ¯ã€‚<br>9. <strong>é€‰æ‹©æ”¯ä»˜æ–¹å¼</strong>ï¼š - é€‰æ‹©æ‚¨å–œæ¬¢çš„æ”¯ä»˜æ–¹å¼ï¼ˆå¦‚æ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ã€é“¶è¡Œå¡ç­‰ï¼‰ã€‚<br>10. <strong>ç¡®è®¤è®¢å•å¹¶æ”¯ä»˜</strong>ï¼š - ç¡®è®¤è®¢å•ä¿¡æ¯æ— è¯¯åï¼Œç‚¹å‡»â€œæ”¯ä»˜â€æŒ‰é’®å®Œæˆæ”¯ä»˜ã€‚<br>11. <strong>æ¥æ”¶ç”µå­æœºç¥¨</strong>ï¼š - æ”¯ä»˜æˆåŠŸåï¼Œæ‚¨å°†æ”¶åˆ°ç”µå­æœºç¥¨ï¼Œå¯ä»¥é€šè¿‡Appå†…çš„â€œæˆ‘çš„è®¢å•â€æŸ¥çœ‹æˆ–ä¸‹è½½ã€‚<br>é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œæ‚¨å°±å¯ä»¥ä½¿ç”¨æºç¨‹Appè½»æ¾è´­ä¹°æœºç¥¨äº†ã€‚</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/7.webp height=128px></figure>æ ¹æ®å›¾è¡¨è®¡ç®—æ ·æœ¬ä¹‹é—´çš„ Î”ct åœ¨å“ªä¸€ä¸ªå¼•ç‰©ä¸‹ç›¸å·®æœ€å°ï¼Ÿ | æ ¹æ®å›¾è¡¨è®¡ç®—æ ·æœ¬ä¹‹é—´çš„ Î”ct åœ¨ cGAS-qF+qR å¼•ç‰©ä¸‹ç›¸å·®æœ€å°ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼šäº’è”ç½‘å…¬å¼€-èˆ’æ¡ç§‘æŠ€å…¬ä¼—å·ï¼‰</div><div class=role>Ming-Lite-Omni</div><div class=content>æ ¹æ®å›¾è¡¨è®¡ç®—æ ·æœ¬ä¹‹é—´çš„ Î”ct åœ¨ cGAS-qF+qR å¼•ç‰©ä¸‹ç›¸å·®æœ€å°ã€‚</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>åœ¨ <strong>EGFP-cGASå•å…‹éš†</strong> å¼•ç‰©ä¸‹ï¼Œæ ·æœ¬ä¹‹é—´çš„ Î”ct ç›¸å·®æœ€å°ã€‚<br><br><strong>è®¡ç®—è¿‡ç¨‹ï¼š</strong><br><strong>1. ç†è§£ Î”ct çš„å«ä¹‰ï¼š</strong> Î”ct æ˜¯ç›®æ ‡åŸºå› ï¼ˆcGAS-qF+qR æˆ– EGFP-cGASï¼‰ä¸å†…å‚åŸºå› ï¼ˆcTubulinï¼‰çš„ç›¸å¯¹è¡¨è¾¾é‡å·®å€¼ã€‚Î”ct è¶Šå°ï¼Œè¯´æ˜ç›®æ ‡åŸºå› ä¸å†…å‚åŸºå› çš„è¡¨è¾¾é‡å·®å¼‚è¶Šå°ã€‚<br><strong>2. æ¯”è¾ƒä¸åŒå¼•ç‰©ä¸‹çš„ Î”ctï¼š</strong><br>- <strong>cGAS-qF+qR å¼•ç‰©ï¼š</strong> - é‡ç”Ÿå‹ï¼šÎ”ct = 8.06 - EGFP-cGASå•å…‹éš†ï¼šÎ”ct = -0.15 - å·®å¼‚ï¼š8.06 - (-0.15) = 8.21<br>- <strong>EGFP-cGASå•å…‹éš† å¼•ç‰©ï¼š</strong> - é‡ç”Ÿå‹ï¼šÎ”ct = 10.44 - EGFP-cGASå•å…‹éš†ï¼šÎ”ct = -2.9 - å·®å¼‚ï¼š10.44 - (-2.9) = 13.34<br>- <strong>EGFP-cGAS-qF+qR å¼•ç‰©ï¼š</strong> - é‡ç”Ÿå‹ï¼šÎ”ct = 21.43 - EGFP-cGASå•å…‹éš†ï¼šÎ”ct = -13.45 - å·®å¼‚ï¼š21.43 - (-13.45) = 34.88<br><strong>3. ç»“è®ºï¼š</strong> åœ¨ <strong>EGFP-cGASå•å…‹éš†</strong> å¼•ç‰©ä¸‹ï¼Œæ ·æœ¬ä¹‹é—´çš„ Î”ct å·®å¼‚æœ€å°ï¼ˆÎ”ct å·®å¼‚ä¸º 13.34ï¼‰ï¼Œè¯´æ˜è¯¥å¼•ç‰©ä¸‹æ ·æœ¬ä¹‹é—´çš„è¡¨è¾¾é‡å·®å¼‚æœ€å°ã€‚<br><br><strong>æ€»ç»“ï¼š</strong> åœ¨ <strong>EGFP-cGASå•å…‹éš†</strong> å¼•ç‰©ä¸‹ï¼Œæ ·æœ¬ä¹‹é—´çš„ Î”ct ç›¸å·®æœ€å°ã€‚</div></div></div></div><h3 id=speech-generation>Speech Generation<a hidden class=anchor aria-hidden=true href=#speech-generation>#</a></h3><table><thead><tr><th>Text Inputs</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Ming-lite-omni V1.5</th></tr></thead><tbody><tr><td>Tyler, Lucy, Michelle, we&rsquo;re going to space!</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en1_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en1_v1.5.wav type=audio/wav></audio></td></tr><tr><td>what is the forecast for California for rain</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en2_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en2_v1.5.wav type=audio/wav></audio></td></tr><tr><td>Television can make you dumb, but it can also be good education.</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en3_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en3_v1.5.wav type=audio/wav></audio></td></tr></tbody></table><div class="full-width-container example-container"><div class=example-content><div class=title><span>OCR</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/9.webp height=320px></figure>åœ¨åˆ¤æ–­é¢˜ç¬¬95é¢˜ä¸­ï¼Œè€ƒç”Ÿç­”é¢˜çš„æ‰‹å†™å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿè‹¥è€ƒç”Ÿçš„æ‰‹å†™ç­”æ¡ˆä¸ºâˆšåˆ™è¾“å‡ºå¯¹ï¼Œè‹¥è€ƒç”Ÿçš„æ‰‹å†™ç­”æ¡ˆä¸ºÃ—åˆ™è¾“å‡ºé”™ï¼Œè¯·è¾“å‡ºå¯¹æˆ–é”™</div><div class=role>Ming-Lite-Omni</div><div class=content>å¯¹</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>é”™</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/10.webp height=64px></figure>Output the content of the CAPTCHA directly. Characters are only numbers and uppercase/lowercase letters.</div><div class=role>Ming-Lite-Omni</div><div class=content>9743</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>9742</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/11.webp height=64px></figure>What is the Latex tag for mathematical expression in images?</div><div class=role>Ming-Lite-Omni</div><div class=content>A C = C B \therefore C D / / B E</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>A C = C B \because C D / / B E</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/12.webp height=320px></figure>ä»å›¾ä¸­æå–: æ—¥æœŸ</div><div class=role>Ming-Lite-Omni</div><div class=content>æ—¥æœŸï¼š2019å¹´12æœˆ03æ—¥05æ—¶08åˆ† ä»å›¾ä¸­æå–: é‡Œç¨‹ é‡Œç¨‹ï¼š23.9å…¬é‡Œ ä»å›¾ä¸­æå–: é‡‘é¢ é‡‘é¢ï¼š52.6å…ƒ ä»å›¾ä¸­æå–: çŠ¶æ€ çŠ¶æ€ï¼š ä»å›¾ä¸­æå–: å®¢ç¥¨å°åŠ¡ å®¢ç¥¨å°åŠ¡2019å¹´12æœˆå°900000å· ä»å›¾ä¸­æå–: å·ç  å·ç ä»00000001è‡³90000000æ­¢</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>æ—¥æœŸ: 2021å¹´03æœˆ05æ—¥</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/13.webp height=320px></figure>What is the difference in value between Lamb and Corn?&rsquo; You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within [think] [/think] tags. The final answer MUST BE put in \boxed{}.'</div><div class=role>Ming-Lite-Omni</div><div class=content>0.37</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>[think]<br>The value for Lamb is 103.7 and for Corn is 103.13. To find the difference, subtract the smaller value from the larger value: 103.7 - 103.13 = 0.57.<br>[/think]<br>\boxed{0.57}</div></div></div></div><h2 id=get-started-with-ming-lite-omni-v15>Get Started with Ming-lite-omni v1.5<a hidden class=anchor aria-hidden=true href=#get-started-with-ming-lite-omni-v15>#</a></h2><p>The model and code for Ming-lite-omni v1.5 are now open-source, and we invite everyone to try it out, share feedback, and join the discussion. Looking ahead, we&rsquo;re excited to announce that a quantized and accelerated version is on the way. This future release will not only further enhance omni-modal performance but also make the model even more lightweight, all while strengthening its multimodal reasoning and generation capabilities. Stay tuned for more updates!</p><ul><li>Github: <a href=https://github.com/inclusionAI/Ming>https://github.com/inclusionAI/Ming</a></li><li>Hugging Face: <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5</a></li><li>ModelScope: <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5</a></li></ul></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://Biao-Gong.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>