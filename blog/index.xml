<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on INCLUSION AI</title>
    <link>https://Biao-Gong.github.io/blog/</link>
    <description>Recent content in Blog on INCLUSION AI</description>
    <image>
      <url>https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 14 Jul 2025 00:00:03 +0800</lastBuildDate><atom:link href="https://Biao-Gong.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ming-Lite-Omni V1.5</title>
      <link>https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/</link>
      <pubDate>Mon, 14 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/</guid>
      <description>【英文版待更新】 GITHUB Ming-Lite-Omni V1.5 整体评测结果 复杂文档理解 图文及体验 定义视频理解新标杆 在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 Ming-Omni-Lite 在多项核心视频理解基准测试中取得了突破性进展。
性能 我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：
评测基准 Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 技术背后 Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：
高效的时空建模器： 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。 高质量、多样化的视频-文本对齐数据： 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。 创新的训练目标与课程学习： 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。 迈向更智能的视频交互 Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理复杂、长时间、信息密集视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。</description>
    </item>
    
    <item>
      <title>M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning</title>
      <link>https://Biao-Gong.github.io/blog/m2-reasoning/</link>
      <pubDate>Fri, 11 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/m2-reasoning/</guid>
      <description>📖 Technical Report | 🤗 Hugging Face｜ 🤖 ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals.</description>
    </item>
    
    <item>
      <title>ABench: An Evolving Open-Source Benchmark</title>
      <link>https://Biao-Gong.github.io/blog/abench/</link>
      <pubDate>Tue, 08 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/abench/</guid>
      <description>GITHUB 🌟 Overview ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
🎯 Core Objectives Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems 📊 Dataset Release Status Domain Description Status Physics 500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics ✅ Released Actuary Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management ✅ Released Logic High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam) 🔄 In Preparation Psychology Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories 🔄 In Preparation Law Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law 🔄 In Preparation </description>
    </item>
    
    <item>
      <title>AWorld: The Agent Runtime for Self-Improvement</title>
      <link>https://Biao-Gong.github.io/blog/aworld/</link>
      <pubDate>Mon, 07 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/aworld/</guid>
      <description>&amp;ldquo;Self-awareness: the hardest problem isn&amp;rsquo;t solving within limits, it&amp;rsquo;s discovering the own limitations&amp;rdquo; Table of Contents News — Latest updates and announcements. Introduction — Overview and purpose of the project. Installation — Step-by-step setup instructions. Quick Start — Get started with usage examples. Architecture — Explore the multi-agent system design. Demo — See the project in action with demonstrations. Contributing — How to get involved and contribute. License — Project licensing details.</description>
    </item>
    
    <item>
      <title>Ming-Omni: A Unified Multimodal Model for Perception and Generation</title>
      <link>https://Biao-Gong.github.io/blog/ming-omni/</link>
      <pubDate>Wed, 11 Jun 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-omni/</guid>
      <description>GITHUB 📑 Technical Report｜📖Project Page ｜🤗 Hugging Face｜ 🤖 ModelScope
Introduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers.</description>
    </item>
    
    <item>
      <title>Ling: A MoE LLM Provided and Open-sourced by InclusionAI</title>
      <link>https://Biao-Gong.github.io/blog/ling/</link>
      <pubDate>Thu, 08 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ling/</guid>
      <description>🤗 Hugging Face&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp🤖 ModelScope
Introduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.
Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems.</description>
    </item>
    
    <item>
      <title>Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</title>
      <link>https://Biao-Gong.github.io/blog/ming-lite-uni/</link>
      <pubDate>Wed, 07 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-lite-uni/</guid>
      <description>GITHUB 📑 Paper｜🤗 Hugging Face｜🤖 ModelScope Introduction Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.
This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension.</description>
    </item>
    
    <item>
      <title>Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities</title>
      <link>https://Biao-Gong.github.io/blog/ming-lite-omni-preview/</link>
      <pubDate>Mon, 05 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-lite-omni-preview/</guid>
      <description>GITHUB 🤗 Hugging Face | 🤖 ModelScope
Introduction Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable.</description>
    </item>
    
    <item>
      <title>Agentic Learning</title>
      <link>https://Biao-Gong.github.io/blog/agenticlearning/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/agenticlearning/</guid>
      <description>Introduction Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment. For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.
We introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively.</description>
    </item>
    
    <item>
      <title>AReaL: Ant Reasoning Reinforcement Learning for LLMs</title>
      <link>https://Biao-Gong.github.io/blog/areal/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/areal/</guid>
      <description>| Paper | Documentation | Ask DeepWiki | 🤗 Models &amp; Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably.</description>
    </item>
    
    <item>
      <title>PromptCoT &amp; PromptCoT-Mamba: Advancing the Frontiers of Reasoning</title>
      <link>https://Biao-Gong.github.io/blog/promptcot/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/promptcot/</guid>
      <description>News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba.</description>
    </item>
    
    <item>
      <title>Ring: A Reasoning MoE LLM Provided and Open-sourced by InclusionAI</title>
      <link>https://Biao-Gong.github.io/blog/ring/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ring/</guid>
      <description>🤗 Hugging Face&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp🤖 ModelScope News [2025-06]:🎉 Add Ring-lite Model [2025-04]:🎉 Add Ring-lite-linear-preview Model Introduction Ring is a reasoning MoE LLM provided and open-sourced by InclusionAI, derived from Ling. We introduce Ring-lite-distill-preview, which has 16.8 billion parameters with 2.75 billion activated parameters. This model demonstrates impressive reasoning performance compared to existing models in the industry.
Model Downloads You can download the following table to see the various parameters for your use case.</description>
    </item>
    
  </channel>
</rss>
