<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on INCLUSION AI</title>
    <link>https://Biao-Gong.github.io/blog/</link>
    <description>Recent content in Blog on INCLUSION AI</description>
    <image>
      <url>https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 14 Jul 2025 00:00:03 +0800</lastBuildDate><atom:link href="https://Biao-Gong.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ming-Lite-Omni V1.5</title>
      <link>https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/</link>
      <pubDate>Mon, 14 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/</guid>
      <description>ã€è‹±æ–‡ç‰ˆå¾…æ›´æ–°ã€‘ GITHUB Ming-Lite-Omni V1.5 æ•´ä½“è¯„æµ‹ç»“æœ å¤æ‚æ–‡æ¡£ç†è§£ å›¾æ–‡åŠä½“éªŒ å®šä¹‰è§†é¢‘ç†è§£æ–°æ ‡æ† åœ¨è¿½æ±‚é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„é“è·¯ä¸Šï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯¹è§†é¢‘å†…å®¹çš„ç†è§£èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç°å®ä¸–ç•Œçš„ä¿¡æ¯æ˜¯åŠ¨æ€ã€è¿ç»­çš„ï¼Œè§†é¢‘æ‰¿è½½ç€è¿œè¶…é™æ€å›¾åƒçš„ä¸°å¯Œæ—¶ç©ºè¯­ä¹‰ã€‚ Ming-Omni-Lite åœ¨å¤šé¡¹æ ¸å¿ƒè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
æ€§èƒ½ æˆ‘ä»¬é€‰å–äº†å½“å‰æœ€å…·ä»£è¡¨æ€§å’ŒæŒ‘æˆ˜æ€§çš„è§†é¢‘ç†è§£åŸºå‡†ï¼Œå°† Ming-Omni-Lite ä¸ä¸šç•Œé¡¶å°–çš„åŒä½“é‡æ¨¡å‹ï¼ˆQwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8Bï¼‰è¿›è¡Œäº†å…¨é¢å¯¹æ¯”ã€‚ç»“æœè¯„æµ‹ç»“æœå±•ç¤ºäº† Ming-Omni-Lite çš„å“è¶Šæ€§èƒ½ï¼š
è¯„æµ‹åŸºå‡† Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 æŠ€æœ¯èƒŒå Ming-Omni-Lite åœ¨è§†é¢‘ç†è§£ï¼Œå°¤å…¶æ˜¯é•¿è§†é¢‘ç†è§£ä¸Šçš„çªç ´ï¼Œæºäºæˆ‘ä»¬åœ¨æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ä¸Šçš„å¤šé¡¹åˆ›æ–°ï¼š
é«˜æ•ˆçš„æ—¶ç©ºå»ºæ¨¡å™¨ï¼š åŠ å…¥3D RoPEï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰è§†é¢‘å¸§å†…ï¼ˆç©ºé—´ï¼‰å’Œå¸§é—´ï¼ˆæ—¶é—´ï¼‰çš„ä¾èµ–å…³ç³»ï¼Œæå–å…³é”®çš„åŠ¨æ€ä¿¡æ¯ã€‚ é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†é¢‘-æ–‡æœ¬å¯¹é½æ•°æ®ï¼š æ„å»ºäº†å¤§è§„æ¨¡ã€æ¶µç›–ä¸°å¯Œåœºæ™¯å’Œä»»åŠ¡çš„é•¿/çŸ­è§†é¢‘-æ–‡æœ¬å¯¹æ•°æ®é›†ä»¥åŠTPOï¼ˆtask-perference optimizationï¼‰æ•°æ®ï¼ŒåŒ…æ‹¬æ—¶é—´æ£€ç´¢ä»¥åŠè§†é¢‘è·Ÿè¸ªã€‚æˆ‘ä»¬è¿›è¡Œäº†ç²¾ç»†æ¸…æ´—ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ åˆ°ç²¾å‡†çš„å¯¹é½èƒ½åŠ›ã€‚ åˆ›æ–°çš„è®­ç»ƒç›®æ ‡ä¸è¯¾ç¨‹å­¦ä¹ ï¼š ç»“åˆäº†è§†é¢‘ç‰¹æœ‰çš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒç›®æ ‡ï¼Œå¹¶é‡‡ç”¨ä»çŸ­åˆ°é•¿çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥æå‡æ¨¡å‹å¤„ç†é•¿è§†é¢‘çš„å¤æ‚åº¦ã€‚ è¿ˆå‘æ›´æ™ºèƒ½çš„è§†é¢‘äº¤äº’ Ming-Omni-Lite åœ¨å’ŒåŒå°ºå¯¸SOTAæ¨¡å‹çš„è§†é¢‘ç†è§£åŸºå‡†è¯„æµ‹ä¸Šä¿æŒé¢†å…ˆï¼Œå®ƒè¯æ˜äº† Ming-Omni-Lite å…·å¤‡å¤„ç†å¤æ‚ã€é•¿æ—¶é—´ã€ä¿¡æ¯å¯†é›†è§†é¢‘å†…å®¹çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºè§†é¢‘æ‘˜è¦ã€é•¿è§†é¢‘é—®ç­”ã€æ™ºèƒ½æ•™å­¦ã€è§†é¢‘å†…å®¹å®¡æ ¸ã€äººæœºäº¤äº’ç­‰å¹¿æ³›åº”ç”¨åœºæ™¯å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚æˆ‘ä»¬å°†æŒç»­æŠ•å…¥ç ”å‘ï¼Œè¿›ä¸€æ­¥é‡Šæ”¾ Ming-Omni-Lite åœ¨è§†é¢‘ä¹ƒè‡³å¤šæ¨¡æ€é¢†åŸŸçš„æ½œåŠ›ï¼Œè‡´åŠ›äºæ‰“é€ èƒ½å¤ŸçœŸæ­£ç†è§£ã€æ¨ç†å’Œä¸ç°å®ä¸–ç•Œäº¤äº’çš„æ™ºèƒ½ä½“ã€‚</description>
    </item>
    
    <item>
      <title>M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning</title>
      <link>https://Biao-Gong.github.io/blog/m2-reasoning/</link>
      <pubDate>Fri, 11 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/m2-reasoning/</guid>
      <description>ğŸ“– Technical Report | ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals.</description>
    </item>
    
    <item>
      <title>ABench: An Evolving Open-Source Benchmark</title>
      <link>https://Biao-Gong.github.io/blog/abench/</link>
      <pubDate>Tue, 08 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/abench/</guid>
      <description>GITHUB ğŸŒŸ Overview ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
ğŸ¯ Core Objectives Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems ğŸ“Š Dataset Release Status Domain Description Status Physics 500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics âœ… Released Actuary Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management âœ… Released Logic High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam) ğŸ”„ In Preparation Psychology Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories ğŸ”„ In Preparation Law Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law ğŸ”„ In Preparation </description>
    </item>
    
    <item>
      <title>AWorld: The Agent Runtime for Self-Improvement</title>
      <link>https://Biao-Gong.github.io/blog/aworld/</link>
      <pubDate>Mon, 07 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/aworld/</guid>
      <description>&amp;ldquo;Self-awareness: the hardest problem isn&amp;rsquo;t solving within limits, it&amp;rsquo;s discovering the own limitations&amp;rdquo; Table of Contents News â€” Latest updates and announcements. Introduction â€” Overview and purpose of the project. Installation â€” Step-by-step setup instructions. Quick Start â€” Get started with usage examples. Architecture â€” Explore the multi-agent system design. Demo â€” See the project in action with demonstrations. Contributing â€” How to get involved and contribute. License â€” Project licensing details.</description>
    </item>
    
    <item>
      <title>Ming-Omni: A Unified Multimodal Model for Perception and Generation</title>
      <link>https://Biao-Gong.github.io/blog/ming-omni/</link>
      <pubDate>Wed, 11 Jun 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-omni/</guid>
      <description>GITHUB ğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers.</description>
    </item>
    
    <item>
      <title>Ling: A MoE LLM Provided and Open-sourced by InclusionAI</title>
      <link>https://Biao-Gong.github.io/blog/ling/</link>
      <pubDate>Thu, 08 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ling/</guid>
      <description>ğŸ¤— Hugging Face&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspğŸ¤– ModelScope
Introduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.
Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems.</description>
    </item>
    
    <item>
      <title>Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</title>
      <link>https://Biao-Gong.github.io/blog/ming-lite-uni/</link>
      <pubDate>Wed, 07 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-lite-uni/</guid>
      <description>GITHUB ğŸ“‘ Paperï½œğŸ¤— Hugging Faceï½œğŸ¤– ModelScope Introduction Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.
This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension.</description>
    </item>
    
    <item>
      <title>Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities</title>
      <link>https://Biao-Gong.github.io/blog/ming-lite-omni-preview/</link>
      <pubDate>Mon, 05 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-lite-omni-preview/</guid>
      <description>GITHUB ğŸ¤— Hugging Face | ğŸ¤– ModelScope
Introduction Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable.</description>
    </item>
    
    <item>
      <title>Agentic Learning</title>
      <link>https://Biao-Gong.github.io/blog/agenticlearning/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/agenticlearning/</guid>
      <description>Introduction Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment. For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.
We introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively.</description>
    </item>
    
    <item>
      <title>AReaL: Ant Reasoning Reinforcement Learning for LLMs</title>
      <link>https://Biao-Gong.github.io/blog/areal/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/areal/</guid>
      <description>| Paper | Documentation | Ask DeepWiki | ğŸ¤— Models &amp; Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably.</description>
    </item>
    
    <item>
      <title>PromptCoT &amp; PromptCoT-Mamba: Advancing the Frontiers of Reasoning</title>
      <link>https://Biao-Gong.github.io/blog/promptcot/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/promptcot/</guid>
      <description>News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba.</description>
    </item>
    
    <item>
      <title>Ring: A Reasoning MoE LLM Provided and Open-sourced by InclusionAI</title>
      <link>https://Biao-Gong.github.io/blog/ring/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ring/</guid>
      <description>ğŸ¤— Hugging Face&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspğŸ¤– ModelScope News [2025-06]:ğŸ‰ Add Ring-lite Model [2025-04]:ğŸ‰ Add Ring-lite-linear-preview Model Introduction Ring is a reasoning MoE LLM provided and open-sourced by InclusionAI, derived from Ling. We introduce Ring-lite-distill-preview, which has 16.8 billion parameters with 2.75 billion activated parameters. This model demonstrates impressive reasoning performance compared to existing models in the industry.
Model Downloads You can download the following table to see the various parameters for your use case.</description>
    </item>
    
  </channel>
</rss>
