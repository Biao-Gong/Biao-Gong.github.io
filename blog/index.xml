<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on INCLUSION AI</title>
    <link>https://Biao-Gong.github.io/blog/</link>
    <description>Recent content in Blog on INCLUSION AI</description>
    <image>
      <url>https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 07 Jul 2025 00:00:04 +0800</lastBuildDate><atom:link href="https://Biao-Gong.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[ Markdown Template by Gong Biao ]</title>
      <link>https://Biao-Gong.github.io/blog/template/</link>
      <pubDate>Mon, 07 Jul 2025 00:00:04 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/template/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Note: This is the pronunciation of QwQ: /kwju:/ , similar to the word &amp;ldquo;quill&amp;rdquo;.
What does it mean to think, to question, to understand? These are the deep waters that QwQ (Qwen with Questions) wades into. Like an eternal student of wisdom, it approaches every problem - be it mathematics, code, or knowledge of our world - with genuine wonder and doubt. QwQ embodies that ancient philosophical spirit: it knows that it knows nothing, and that&amp;rsquo;s precisely what drives its curiosity.</description>
    </item>
    
    <item>
      <title>Ming-Omni: A Unified Multimodal Model for Perception and Generation</title>
      <link>https://Biao-Gong.github.io/blog/ming-omni/</link>
      <pubDate>Wed, 11 Jun 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-omni/</guid>
      <description>GITHUB ðŸ“‘ Technical Reportï½œðŸ“–Project Page ï½œðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope Introduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers.</description>
    </item>
    
    <item>
      <title>Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</title>
      <link>https://Biao-Gong.github.io/blog/ming-lite-uni/</link>
      <pubDate>Wed, 07 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-lite-uni/</guid>
      <description>GITHUB ðŸ“‘ Paperï½œðŸ¤— Hugging Faceï½œðŸ¤– ModelScope Introduction Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.
This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension.</description>
    </item>
    
    <item>
      <title>Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities</title>
      <link>https://Biao-Gong.github.io/blog/ming-lite-omni-preview/</link>
      <pubDate>Tue, 06 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://Biao-Gong.github.io/blog/ming-lite-omni-preview/</guid>
      <description>GITHUB ðŸ¤— Hugging Face | ðŸ¤– ModelScope Introduction Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable.</description>
    </item>
    
  </channel>
</rss>
