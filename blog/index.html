<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://Biao-Gong.github.io/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://Biao-Gong.github.io/favicon.png><link rel=apple-touch-icon href=https://Biao-Gong.github.io/favicon.png><link rel=manifest href=https://Biao-Gong.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://Biao-Gong.github.io/blog/index.xml><link rel=alternate hreflang=en href=https://Biao-Gong.github.io/blog/><link rel=alternate hreflang=zh href=https://Biao-Gong.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://Biao-Gong.github.io/blog/"><meta property="og:image" content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://Biao-Gong.github.io/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://Biao-Gong.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span class=active>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#FFCC99 0%,#7F667F 40%,#262656 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://Biao-Gong.github.io/zh/blog/>ç®€ä½“ä¸­æ–‡</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Ming-Lite-Omni V1.5</h2></header><div class=entry-content><p>ã€è‹±æ–‡ç‰ˆå¾…æ›´æ–°ã€‘ GITHUB Ming-Lite-Omni V1.5 æ•´ä½“è¯„æµ‹ç»“æœ å¤æ‚æ–‡æ¡£ç†è§£ å›¾æ–‡åŠä½“éªŒ å®šä¹‰è§†é¢‘ç†è§£æ–°æ ‡æ† åœ¨è¿½æ±‚é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„é“è·¯ä¸Šï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯¹è§†é¢‘å†…å®¹çš„ç†è§£èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç°å®ä¸–ç•Œçš„ä¿¡æ¯æ˜¯åŠ¨æ€ã€è¿ç»­çš„ï¼Œè§†é¢‘æ‰¿è½½ç€è¿œè¶…é™æ€å›¾åƒçš„ä¸°å¯Œæ—¶ç©ºè¯­ä¹‰ã€‚ Ming-Omni-Lite åœ¨å¤šé¡¹æ ¸å¿ƒè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
æ€§èƒ½ æˆ‘ä»¬é€‰å–äº†å½“å‰æœ€å…·ä»£è¡¨æ€§å’ŒæŒ‘æˆ˜æ€§çš„è§†é¢‘ç†è§£åŸºå‡†ï¼Œå°† Ming-Omni-Lite ä¸ä¸šç•Œé¡¶å°–çš„åŒä½“é‡æ¨¡å‹ï¼ˆQwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8Bï¼‰è¿›è¡Œäº†å…¨é¢å¯¹æ¯”ã€‚ç»“æœè¯„æµ‹ç»“æœå±•ç¤ºäº† Ming-Omni-Lite çš„å“è¶Šæ€§èƒ½ï¼š
è¯„æµ‹åŸºå‡† Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 æŠ€æœ¯èƒŒå Ming-Omni-Lite åœ¨è§†é¢‘ç†è§£ï¼Œå°¤å…¶æ˜¯é•¿è§†é¢‘ç†è§£ä¸Šçš„çªç ´ï¼Œæºäºæˆ‘ä»¬åœ¨æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ä¸Šçš„å¤šé¡¹åˆ›æ–°ï¼š
é«˜æ•ˆçš„æ—¶ç©ºå»ºæ¨¡å™¨ï¼š åŠ å…¥3D RoPEï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰è§†é¢‘å¸§å†…ï¼ˆç©ºé—´ï¼‰å’Œå¸§é—´ï¼ˆæ—¶é—´ï¼‰çš„ä¾èµ–å…³ç³»ï¼Œæå–å…³é”®çš„åŠ¨æ€ä¿¡æ¯ã€‚ é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†é¢‘-æ–‡æœ¬å¯¹é½æ•°æ®ï¼š æ„å»ºäº†å¤§è§„æ¨¡ã€æ¶µç›–ä¸°å¯Œåœºæ™¯å’Œä»»åŠ¡çš„é•¿/çŸ­è§†é¢‘-æ–‡æœ¬å¯¹æ•°æ®é›†ä»¥åŠTPOï¼ˆtask-perference optimizationï¼‰æ•°æ®ï¼ŒåŒ…æ‹¬æ—¶é—´æ£€ç´¢ä»¥åŠè§†é¢‘è·Ÿè¸ªã€‚æˆ‘ä»¬è¿›è¡Œäº†ç²¾ç»†æ¸…æ´—ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ åˆ°ç²¾å‡†çš„å¯¹é½èƒ½åŠ›ã€‚ åˆ›æ–°çš„è®­ç»ƒç›®æ ‡ä¸è¯¾ç¨‹å­¦ä¹ ï¼š ç»“åˆäº†è§†é¢‘ç‰¹æœ‰çš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒç›®æ ‡ï¼Œå¹¶é‡‡ç”¨ä»çŸ­åˆ°é•¿çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥æå‡æ¨¡å‹å¤„ç†é•¿è§†é¢‘çš„å¤æ‚åº¦ã€‚ è¿ˆå‘æ›´æ™ºèƒ½çš„è§†é¢‘äº¤äº’ Ming-Omni-Lite åœ¨å’ŒåŒå°ºå¯¸SOTAæ¨¡å‹çš„è§†é¢‘ç†è§£åŸºå‡†è¯„æµ‹ä¸Šä¿æŒé¢†å…ˆï¼Œå®ƒè¯æ˜äº† Ming-Omni-Lite å…·å¤‡å¤„ç†å¤æ‚ã€é•¿æ—¶é—´ã€ä¿¡æ¯å¯†é›†è§†é¢‘å†…å®¹çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºè§†é¢‘æ‘˜è¦ã€é•¿è§†é¢‘é—®ç­”ã€æ™ºèƒ½æ•™å­¦ã€è§†é¢‘å†…å®¹å®¡æ ¸ã€äººæœºäº¤äº’ç­‰å¹¿æ³›åº”ç”¨åœºæ™¯å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚æˆ‘ä»¬å°†æŒç»­æŠ•å…¥ç ”å‘ï¼Œè¿›ä¸€æ­¥é‡Šæ”¾ Ming-Omni-Lite åœ¨è§†é¢‘ä¹ƒè‡³å¤šæ¨¡æ€é¢†åŸŸçš„æ½œåŠ›ï¼Œè‡´åŠ›äºæ‰“é€ èƒ½å¤ŸçœŸæ­£ç†è§£ã€æ¨ç†å’Œä¸ç°å®ä¸–ç•Œäº¤äº’çš„æ™ºèƒ½ä½“ã€‚...</p></div><footer class=entry-footer><span title='2025-07-14 00:00:03 +0800 +0800'>July 14, 2025</span>&nbsp;Â·&nbsp;2 min&nbsp;Â·&nbsp;284 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-Lite-Omni V1.5" href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/></a></article><article class=post-entry><header class=entry-header><h2>M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning</h2></header><div class=entry-content><p>ğŸ“– Technical Report | ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals....</p></div><footer class=entry-footer><span title='2025-07-11 00:00:03 +0800 +0800'>July 11, 2025</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;1052 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning" href=https://Biao-Gong.github.io/blog/m2-reasoning/></a></article><article class=post-entry><header class=entry-header><h2>ABench: An Evolving Open-Source Benchmark</h2></header><div class=entry-content><p>GITHUB ğŸŒŸ Overview ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
ğŸ¯ Core Objectives Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems ğŸ“Š Dataset Release Status Domain Description Status Physics 500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics âœ… Released Actuary Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management âœ… Released Logic High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam) ğŸ”„ In Preparation Psychology Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories ğŸ”„ In Preparation Law Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law ğŸ”„ In Preparation</p></div><footer class=entry-footer><span title='2025-07-08 00:00:03 +0800 +0800'>July 8, 2025</span>&nbsp;Â·&nbsp;1 min&nbsp;Â·&nbsp;185 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to ABench: An Evolving Open-Source Benchmark" href=https://Biao-Gong.github.io/blog/abench/></a></article><article class=post-entry><header class=entry-header><h2>AWorld: The Agent Runtime for Self-Improvement</h2></header><div class=entry-content><p>â€œSelf-awareness: the hardest problem isnâ€™t solving within limits, itâ€™s discovering the own limitationsâ€ Table of Contents News â€” Latest updates and announcements. Introduction â€” Overview and purpose of the project. Installation â€” Step-by-step setup instructions. Quick Start â€” Get started with usage examples. Architecture â€” Explore the multi-agent system design. Demo â€” See the project in action with demonstrations. Contributing â€” How to get involved and contribute. License â€” Project licensing details....</p></div><footer class=entry-footer><span title='2025-07-07 00:00:03 +0800 +0800'>July 7, 2025</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;895 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to AWorld: The Agent Runtime for Self-Improvement" href=https://Biao-Gong.github.io/blog/aworld/></a></article><article class=post-entry><header class=entry-header><h2>Ming-Omni: A Unified Multimodal Model for Perception and Generation</h2></header><div class=entry-content><p>GITHUB ğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers....</p></div><footer class=entry-footer><span title='2025-06-11 00:00:03 +0800 +0800'>June 11, 2025</span>&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;1379 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-Omni: A Unified Multimodal Model for Perception and Generation" href=https://Biao-Gong.github.io/blog/ming-omni/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://Biao-Gong.github.io/blog/page/2/>Next&nbsp;&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://Biao-Gong.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>