<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Resources | inclusionAI</title><meta name=keywords content><meta name=description content="Links In the following, we provide important links for you to refer to our opensource resources.
GITHUB HUGGING FACE MODELSCOPE API
Quick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:
from transformers import AutoModelForCausalLM, AutoTokenizer device = &#34;cuda&#34; # the device to load the model onto model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-7B-Instruct&#34;, device_map=&#34;auto&#34;) tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-7B-Instruct&#34;) prompt = &#34;Give me a short introduction to large language model."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://Biao-Gong.github.io/resources/><link crossorigin=anonymous href=/assets/css/stylesheet.f2c8c5c99d640d6833f4f42ed8c3a7b5281f71b42b0ebb44177bf32898dbaea1.css integrity="sha256-8sjFyZ1kDWgz9PQu2MOntSgfcbQrDrtEF3vzKJjbrqE=" rel="preload stylesheet" as=style><link rel=icon href=https://Biao-Gong.github.io/favicon.png><link rel=apple-touch-icon href=https://Biao-Gong.github.io/favicon.png><link rel=manifest href=https://Biao-Gong.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://Biao-Gong.github.io/resources/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Resources"><meta property="og:description" content="Links In the following, we provide important links for you to refer to our opensource resources.
GITHUB HUGGING FACE MODELSCOPE API
Quick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:
from transformers import AutoModelForCausalLM, AutoTokenizer device = &#34;cuda&#34; # the device to load the model onto model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-7B-Instruct&#34;, device_map=&#34;auto&#34;) tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-7B-Instruct&#34;) prompt = &#34;Give me a short introduction to large language model."><meta property="og:type" content="article"><meta property="og:url" content="https://Biao-Gong.github.io/resources/"><meta property="og:image" content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Resources"><meta name=twitter:description content="Links In the following, we provide important links for you to refer to our opensource resources.
GITHUB HUGGING FACE MODELSCOPE API
Quick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:
from transformers import AutoModelForCausalLM, AutoTokenizer device = &#34;cuda&#34; # the device to load the model onto model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-7B-Instruct&#34;, device_map=&#34;auto&#34;) tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-7B-Instruct&#34;) prompt = &#34;Give me a short introduction to large language model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Resources","item":"https://Biao-Gong.github.io/resources/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Resources","name":"Resources","description":"Links In the following, we provide important links for you to refer to our opensource resources.\nGITHUB HUGGING FACE MODELSCOPE API\nQuick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; # the device to load the model onto model = AutoModelForCausalLM.from_pretrained(\u0026#34;Qwen/Qwen2.5-7B-Instruct\u0026#34;, device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(\u0026#34;Qwen/Qwen2.5-7B-Instruct\u0026#34;) prompt = \u0026#34;Give me a short introduction to large language model.","keywords":[],"articleBody":"Links In the following, we provide important links for you to refer to our opensource resources.\nGITHUB HUGGING FACE MODELSCOPE API\nQuick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer device = \"cuda\" # the device to load the model onto model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", device_map=\"auto\") tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\") prompt = \"Give me a short introduction to large language model.\" messages = [{\"role\": \"user\", \"content\": prompt}] text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) model_inputs = tokenizer([text], return_tensors=\"pt\").to(device) generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True) generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] ","wordCount":"109","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Biao-Gong.github.io/resources/"},"publisher":{"@type":"Organization","name":"inclusionAI","logo":{"@type":"ImageObject","url":"https://Biao-Gong.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://Biao-Gong.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(to top,#a18cd1 0%,#fbc2eb 100%)"></div><div class="hero text-light"><h1 class=post-title>Resources</h1></div></div><main class=main><article class=post-single><div class=post-content><h2 id=links>Links<a hidden class=anchor aria-hidden=true href=#links>#</a></h2><p>In the following, we provide important links for you to refer to our opensource resources.</p><p><a href=https://github.com/QwenLM/Qwen class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://dashscope.aliyun.com/ class="btn external" target=_blank>API</a></p><h2 id=quick-start>Quick Start<a hidden class=anchor aria-hidden=true href=#quick-start>#</a></h2><p>It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span> <span class=c1># the device to load the model onto</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen2.5-7B-Instruct&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen2.5-7B-Instruct&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;Give me a short introduction to large language model.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=p>[</span><span class=n>output_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>output_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generated_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://Biao-Gong.github.io/>inclusionAI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>