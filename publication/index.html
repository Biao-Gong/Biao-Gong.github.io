<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Publication | INCLUSION AI</title><meta name=keywords content><meta name=description content="Selected Publications Inclusion AI, Ant Group. Ming-omni: A unified multimodal model for perception and generation. Technical Report, 2025.
Inclusion AI, Ant Group. Ming-lite-uni: Advancements in unified architecture for natural multimodal interaction. Technical Report, 2025.
Biao Gong, Shuai Tan, Yutong Feng, Xiaoying Xie, Yuyuan Li, Chaochao Chen, Kecheng Zheng, Yujun Shen, and Deli Zhao. Uknow: A unified knowledge protocol with multimodal knowledge graph datasets for reasoning and vision-language pre-training. NeurIPS, 2024."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://Biao-Gong.github.io/publication/><link crossorigin=anonymous href=/assets/css/stylesheet.c89f196d8672b69cf34afeba27d213af1d6900c106ff9315c4a62af441df5c8a.css integrity="sha256-yJ8ZbYZytpzzSv66J9ITrx1pAMEG/5MVxKYq9EHfXIo=" rel="preload stylesheet" as=style><link rel=icon href=https://Biao-Gong.github.io/favicon.png><link rel=apple-touch-icon href=https://Biao-Gong.github.io/favicon.png><link rel=manifest href=https://Biao-Gong.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://Biao-Gong.github.io/publication/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Publication"><meta property="og:description" content="Selected Publications Inclusion AI, Ant Group. Ming-omni: A unified multimodal model for perception and generation. Technical Report, 2025.
Inclusion AI, Ant Group. Ming-lite-uni: Advancements in unified architecture for natural multimodal interaction. Technical Report, 2025.
Biao Gong, Shuai Tan, Yutong Feng, Xiaoying Xie, Yuyuan Li, Chaochao Chen, Kecheng Zheng, Yujun Shen, and Deli Zhao. Uknow: A unified knowledge protocol with multimodal knowledge graph datasets for reasoning and vision-language pre-training. NeurIPS, 2024."><meta property="og:type" content="article"><meta property="og:url" content="https://Biao-Gong.github.io/publication/"><meta property="og:image" content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Publication"><meta name=twitter:description content="Selected Publications Inclusion AI, Ant Group. Ming-omni: A unified multimodal model for perception and generation. Technical Report, 2025.
Inclusion AI, Ant Group. Ming-lite-uni: Advancements in unified architecture for natural multimodal interaction. Technical Report, 2025.
Biao Gong, Shuai Tan, Yutong Feng, Xiaoying Xie, Yuyuan Li, Chaochao Chen, Kecheng Zheng, Yujun Shen, and Deli Zhao. Uknow: A unified knowledge protocol with multimodal knowledge graph datasets for reasoning and vision-language pre-training. NeurIPS, 2024."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publication","item":"https://Biao-Gong.github.io/publication/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Publication","name":"Publication","description":"Selected Publications Inclusion AI, Ant Group. Ming-omni: A unified multimodal model for perception and generation. Technical Report, 2025.\nInclusion AI, Ant Group. Ming-lite-uni: Advancements in unified architecture for natural multimodal interaction. Technical Report, 2025.\nBiao Gong, Shuai Tan, Yutong Feng, Xiaoying Xie, Yuyuan Li, Chaochao Chen, Kecheng Zheng, Yujun Shen, and Deli Zhao. Uknow: A unified knowledge protocol with multimodal knowledge graph datasets for reasoning and vision-language pre-training. NeurIPS, 2024.","keywords":[],"articleBody":"Selected Publications Inclusion AI, Ant Group. Ming-omni: A unified multimodal model for perception and generation. Technical Report, 2025.\nInclusion AI, Ant Group. Ming-lite-uni: Advancements in unified architecture for natural multimodal interaction. Technical Report, 2025.\nBiao Gong, Shuai Tan, Yutong Feng, Xiaoying Xie, Yuyuan Li, Chaochao Chen, Kecheng Zheng, Yujun Shen, and Deli Zhao. Uknow: A unified knowledge protocol with multimodal knowledge graph datasets for reasoning and vision-language pre-training. NeurIPS, 2024.\nZiyuan Huang, Kaixiang Ji, Biao Gong, Zhiwu Qing, Qinglong Zhang, Kecheng Zheng, Jian Wang, Jingdong Chen, and Ming Yang. Accelerating pre-training of multimodal llms via chain-of-sight. NeurIPS, 2024.\nXin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, et al. Skysense: A multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery. CVPR, 2024.\nShuai Tan*, Biao Gong*, Yutong Feng, Kecheng Zheng, Dandan Zheng, Shuwei Shi, Yujun Shen, Jingdong Chen, and Ming Yang. Mimir: Improving video diffusion models for precise text understanding. CVPR, 2025.\nYun-Hao Cao, Kaixiang Ji, Ziyuan Huang, Chuanyang Zheng, Jiajia Liu, Jian Wang, Jingdong Chen, and Ming Yang. Towards better vision-inspired vision-language models. CVPR, 2024.\nYouming Deng, Yansheng Li, Yongjun Zhang, Xiang Xiang, Jian Wang, Jingdong Chen, and Jiayi Ma. Hierarchical memory learning for fine-grained scene graph generation. ECCV, 2022.\nWen Li, Muyuan Fang, Cheng Zou, Biao Gong, Ruobing Zheng, Meng Wang, Jingdong Chen, and Ming Yang. Styletokenizer: Defining image style by a single instance for controlling diffusion models. ECCV, 2024.\nChaochao Chen, Jiaming Zhang, Yizhao Zhang, Li Zhang, Lingjuan Lyu, Yuyuan Li, Biao Gong, and Chenggang Yan. Cure4rec: A benchmark for recommendation unlearning with deeper influence. NeurIPS, 2024\nCanjie Luo, Lianwen Jin, and Jingdong Chen. Siman: exploring self-supervised representation learning of scene text via similarity-aware normalization. CVPR, 2022.\nWen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, and Chunhua Shen. Framer: Interactive frame interpolation. ICLR, 2025.\nWeixiang Hong, Jiangwei Lao, Wang Ren, Jian Wang, Jingdong Chen, and Wei Chu. Training object detectors from scratch: An empirical study in the era of vision transformer. CVPR, 2022.\nShuwei Shi, Biao Gong, Xi Chen, Dandan Zheng, Shuai Tan, Zizheng Yang, Yuyuan Li, Jingwen He, Kecheng Zheng, Jingdong Chen, et al. Motionstone: Decoupled motion intensity modulation with diffusion transformer for image-to-video generation. CVPR, 2025.\nJiangwei Lao, Weixiang Hong, Xin Guo, Yingying Zhang, Jian Wang, Jingdong Chen, and Wei Chu. Simultaneously short-and long-term temporal modeling for semi-supervised video semantic segmentation. CVPR, 2023.\nShuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, and Yinqiang Zheng. Resmaster: Mastering high-resolution image generation via structural and fine-grained guidance. In The 39th Annual AAAI, 2025.\nYingying Zhang, Xin Guo, Jiangwei Lao, Lei Yu, Lixiang Ru, Jian Wang, Guo Ye, Huimei He, Jingdong Chen, and Ming Yang. Poa: Pre-training once for models of all sizes. ECCV, 2024.\nShuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. ICLR, 2025.\nYujie Wei, Shiwei Zhang, Hangjie Yuan, Biao Gong, Longxiang Tang, Xiang Wang, Haonan Qiu, Hengjia Li, Shuai Tan, Yingya Zhang, et al. Dreamrelation: Relation-centric video customization. ICCV, 2025.\n","wordCount":"528","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Biao-Gong.github.io/publication/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://Biao-Gong.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://Biao-Gong.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span class=active>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#FF9933 0%,#5B70AD 50%,#000099 100%)"></div><div class="hero text-light"><h1 class=post-title>Publication</h1></div></div><main class=main><article class=post-single><div class=post-content><h1 id=selected-publications>Selected Publications<a hidden class=anchor aria-hidden=true href=#selected-publications>#</a></h1><ul><li><p>Inclusion AI, Ant Group. Ming-omni: A unified multimodal model for
perception and generation. <strong>Technical Report</strong>, 2025.</p></li><li><p>Inclusion AI, Ant Group. Ming-lite-uni: Advancements in unified architecture for natural multimodal
interaction. <strong>Technical Report</strong>, 2025.</p></li></ul><hr><ul><li><p>Biao Gong, Shuai Tan, Yutong Feng, Xiaoying Xie, Yuyuan Li, Chaochao Chen, Kecheng Zheng, Yujun
Shen, and Deli Zhao. Uknow: A unified knowledge protocol with multimodal knowledge graph datasets
for reasoning and vision-language pre-training. <strong>NeurIPS</strong>, 2024.</p></li><li><p>Ziyuan Huang, Kaixiang Ji, Biao Gong, Zhiwu Qing, Qinglong Zhang, Kecheng Zheng, Jian Wang,
Jingdong Chen, and Ming Yang. Accelerating pre-training of multimodal llms via chain-of-sight. <strong>NeurIPS</strong>, 2024.</p></li><li><p>Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang
Wu, Dingxiang Hu, et al. Skysense: A multi-modal remote sensing foundation model towards universal
interpretation for earth observation imagery. <strong>CVPR</strong>, 2024.</p></li><li><p>Shuai Tan*, Biao Gong*, Yutong Feng, Kecheng Zheng, Dandan Zheng, Shuwei Shi, Yujun Shen, Jingdong
Chen, and Ming Yang. Mimir: Improving video diffusion models for precise text understanding. <strong>CVPR</strong>, 2025.</p></li><li><p>Yun-Hao Cao, Kaixiang Ji, Ziyuan Huang, Chuanyang Zheng, Jiajia Liu, Jian Wang, Jingdong Chen, and
Ming Yang. Towards better vision-inspired vision-language models. <strong>CVPR</strong>, 2024.</p></li><li><p>Youming Deng, Yansheng Li, Yongjun Zhang, Xiang Xiang, Jian Wang, Jingdong Chen, and Jiayi Ma.
Hierarchical memory learning for fine-grained scene graph generation. <strong>ECCV</strong>, 2022.</p></li><li><p>Wen Li, Muyuan Fang, Cheng Zou, Biao Gong, Ruobing Zheng, Meng Wang, Jingdong Chen, and Ming
Yang. Styletokenizer: Defining image style by a single instance for controlling diffusion models. <strong>ECCV</strong>, 2024.</p></li><li><p>Chaochao Chen, Jiaming Zhang, Yizhao Zhang, Li Zhang, Lingjuan Lyu, Yuyuan Li, Biao Gong, and
Chenggang Yan. Cure4rec: A benchmark for recommendation unlearning with deeper influence. <strong>NeurIPS</strong>, 2024</p></li><li><p>Canjie Luo, Lianwen Jin, and Jingdong Chen. Siman: exploring self-supervised representation learning of
scene text via similarity-aware normalization. <strong>CVPR</strong>, 2022.</p></li><li><p>Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen,
and Chunhua Shen. Framer: Interactive frame interpolation. <strong>ICLR</strong>, 2025.</p></li><li><p>Weixiang Hong, Jiangwei Lao, Wang Ren, Jian Wang, Jingdong Chen, and Wei Chu. Training object
detectors from scratch: An empirical study in the era of vision transformer. <strong>CVPR</strong>, 2022.</p></li><li><p>Shuwei Shi, Biao Gong, Xi Chen, Dandan Zheng, Shuai Tan, Zizheng Yang, Yuyuan Li, Jingwen He,
Kecheng Zheng, Jingdong Chen, et al. Motionstone: Decoupled motion intensity modulation with diffusion
transformer for image-to-video generation. <strong>CVPR</strong>, 2025.</p></li><li><p>Jiangwei Lao, Weixiang Hong, Xin Guo, Yingying Zhang, Jian Wang, Jingdong Chen, and Wei Chu.
Simultaneously short-and long-term temporal modeling for semi-supervised video semantic segmentation. <strong>CVPR</strong>, 2023.</p></li><li><p>Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, and Yinqiang Zheng. Resmaster:
Mastering high-resolution image generation via structural and fine-grained guidance. In The 39th Annual
<strong>AAAI</strong>, 2025.</p></li><li><p>Yingying Zhang, Xin Guo, Jiangwei Lao, Lei Yu, Lixiang Ru, Jian Wang, Guo Ye, Huimei He, Jingdong
Chen, and Ming Yang. Poa: Pre-training once for models of all sizes. <strong>ECCV</strong>, 2024.</p></li><li><p>Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng,
Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion
representation. <strong>ICLR</strong>, 2025.</p></li><li><p>Yujie Wei, Shiwei Zhang, Hangjie Yuan, Biao Gong, Longxiang Tang, Xiang Wang, Haonan Qiu, Hengjia
Li, Shuai Tan, Yingya Zhang, et al. Dreamrelation: Relation-centric video customization. <strong>ICCV</strong>, 2025.</p></li></ul></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://Biao-Gong.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>