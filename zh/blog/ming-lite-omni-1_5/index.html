<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Omni V1.5 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中，与业界领先的模型相比展现出极具竞争力的结果。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。
性能对比图
模型架构图
详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。
图像/文本/视频/语音理解
MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 图像/语音生成
图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID & Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见表X。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑，详见图X。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 用户偏好对齐
为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们构建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.5在内部对抗评测集上相比Ming-lite-omni v1 胜和率为 87."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.c89f196d8672b69cf34afeba27d213af1d6900c106ff9315c4a62af441df5c8a.css integrity="sha256-yJ8ZbYZytpzzSv66J9ITrx1pAMEG/5MVxKYq9EHfXIo=" rel="preload stylesheet" as=style><link rel=icon href=https://Biao-Gong.github.io/favicon.png><link rel=apple-touch-icon href=https://Biao-Gong.github.io/favicon.png><link rel=manifest href=https://Biao-Gong.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-Lite-Omni V1.5"><meta property="og:description" content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中，与业界领先的模型相比展现出极具竞争力的结果。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。
性能对比图
模型架构图
详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。
图像/文本/视频/语音理解
MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 图像/语音生成
图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID & Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见表X。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑，详见图X。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 用户偏好对齐
为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们构建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.5在内部对抗评测集上相比Ming-lite-omni v1 胜和率为 87."><meta property="og:type" content="article"><meta property="og:url" content="https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-17T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-17T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Biao-Gong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Omni V1.5"><meta name=twitter:description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中，与业界领先的模型相比展现出极具竞争力的结果。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。
性能对比图
模型架构图
详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。
图像/文本/视频/语音理解
MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 图像/语音生成
图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID & Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见表X。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑，详见图X。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 用户偏好对齐
为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们构建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.5在内部对抗评测集上相比Ming-lite-omni v1 胜和率为 87."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://Biao-Gong.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Omni V1.5","item":"https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Omni V1.5","name":"Ming-Lite-Omni V1.5","description":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\n前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中，与业界领先的模型相比展现出极具竞争力的结果。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。\n性能对比图\n模型架构图\n详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。\n图像/文本/视频/语音理解\nMRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 图像/语音生成\n图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID \u0026amp; Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见表X。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑，详见图X。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 用户偏好对齐\n为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们构建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.5在内部对抗评测集上相比Ming-lite-omni v1 胜和率为 87.","keywords":[],"articleBody":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\n前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中，与业界领先的模型相比展现出极具竞争力的结果。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。\n性能对比图\n模型架构图\n详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。\n图像/文本/视频/语音理解\nMRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 图像/语音生成\n图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID \u0026 Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见表X。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑，详见图X。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 用户偏好对齐\n为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们构建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.5在内部对抗评测集上相比Ming-lite-omni v1 胜和率为 87.07%, 使用体验得到了明显优化。\n评测基准 评测维度 Qwen2.5-VL-7B Ming-Omni-Lite V1.5 体验 均分 4.274 4.365 自建体验评测集_相关性 4.308 4.5 自建体验评测集_流畅性 4.765 4.91 自建体验评测集_内容丰富性 3.828 3.69 自建体验评测集_格式合理性 4.727 4.8 自建体验评测集_正确性 3.741 3.92 开始使用 Ming-lite-omni v1.5 Ming-lite-omni v1.5的模型和代码已开源，欢迎大家试用、反馈和交流。后续我们会持续优化Ming-lite-omni，持续提升在全模态的效果同时，让Ming-lite-omni更加轻量化，同时强化Ming-lite-omni的多模推理能力和生成能力。\nGithub：https://github.com/inclusionAI/Ming Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5 ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5 ","wordCount":"108","inLanguage":"zh","datePublished":"2025-07-17T00:00:03+08:00","dateModified":"2025-07-17T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Biao-Gong.github.io/zh/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://Biao-Gong.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://Biao-Gong.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Lite-Omni V1.5</h1><div class=post-meta><span title='2025-07-17 00:00:03 +0800 +0800'>2025年7月17日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;108 字&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://Biao-Gong.github.io/blog/ming-lite-omni-1_5/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming%20 class="btn external" target=_blank>GITHUB</a> 🤗 <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>Hugging Face</a>｜ 🤖 <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>ModelScope</a></p><h1 id=前言>前言<a hidden class=anchor aria-hidden=true href=#前言>#</a></h1><p>本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中，与业界领先的模型相比展现出极具竞争力的结果。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。</p><div style=text-align:center><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/0.webp alt="Image description"><p style=font-size:14px;color:gray>性能对比图</p></div><div style=text-align:center><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/1.webp alt="Image description"><p style=font-size:14px;color:gray>模型架构图</p></div><h1 id=详细介绍>详细介绍<a hidden class=anchor aria-hidden=true href=#详细介绍>#</a></h1><p>为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。</p><p><strong>图像/文本/视频/语音理解</strong></p><ol><li>MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。</li><li>高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需<u>分步冻结/解冻 LLM 的预训练流程</u>，升级为<u>高效全参数训练</u>，训练周期缩短 26.5%，保持性能无损。</li><li>针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。</li><li>针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。</li><li>全方位数据升级<ul><li>预训练阶段<ul><li>新增文本实体结构化数据，补全图谱盲区。</li><li>扩充高质量商品数据，提升通识能力。</li></ul></li><li>指令微调阶段<ul><li>提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。</li><li>提高垂类识别（动植物/车辆/食材等）数据深度。</li><li>从数据角度优化跨学科复杂图文推理能力。</li><li>针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。</li></ul></li></ul></li></ol><p><strong>图像/语音生成</strong></p><ol><li>图像生成侧采用<u>双分支解耦策略</u>提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。</li><li>为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了<u>ID & Scene Consistency Loss</u>，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。</li><li>引入<u>感知增强策略</u>。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，<strong><font style=background-color:#FBDE28;>详见表X</font></strong>。</li><li>引入<u>多任务协同学习策略</u>。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑，<strong><font style=background-color:#FBDE28;>详见图X</font></strong>。</li><li>语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。</li><li>语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。</li><li>全方位数据升级<ul><li>获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。</li><li>采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。</li><li>构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。</li></ul></li></ol><p><strong>用户偏好对齐</strong></p><p>为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们构建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.5在内部对抗评测集上相比Ming-lite-omni v1 胜和率为 87.07%, 使用体验得到了明显优化。</p><table><thead><tr><th>评测基准</th><th>评测维度</th><th>Qwen2.5-VL-7B</th><th>Ming-Omni-Lite V1.5</th></tr></thead><tbody><tr><td>体验</td><td>均分</td><td>4.274</td><td>4.365</td></tr><tr><td></td><td>自建体验评测集_相关性</td><td>4.308</td><td>4.5</td></tr><tr><td></td><td>自建体验评测集_流畅性</td><td>4.765</td><td>4.91</td></tr><tr><td></td><td>自建体验评测集_内容丰富性</td><td>3.828</td><td>3.69</td></tr><tr><td></td><td>自建体验评测集_格式合理性</td><td>4.727</td><td>4.8</td></tr><tr><td></td><td>自建体验评测集_正确性</td><td>3.741</td><td>3.92</td></tr></tbody></table><h1 id=开始使用-ming-lite-omni-v15>开始使用 Ming-lite-omni v1.5<a hidden class=anchor aria-hidden=true href=#开始使用-ming-lite-omni-v15>#</a></h1><p>Ming-lite-omni v1.5的模型和代码已开源，欢迎大家试用、反馈和交流。后续我们会持续优化Ming-lite-omni，持续提升在全模态的效果同时，让Ming-lite-omni更加轻量化，同时强化Ming-lite-omni的多模推理能力和生成能力。</p><ul><li>Github：https://github.com/inclusionAI/Ming</li><li>Hugging Face: <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5</a></li><li>ModelScope: <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5</a></li></ul></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://Biao-Gong.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>